{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PUNT_GLOBAL</th>\n",
       "      <th>COLE_AREA_UBICACION_URBANO</th>\n",
       "      <th>COLE_BILINGUE_S</th>\n",
       "      <th>COLE_CALENDARIO_B</th>\n",
       "      <th>COLE_CALENDARIO_OTRO</th>\n",
       "      <th>COLE_CARACTER_NO APLICA</th>\n",
       "      <th>COLE_CARACTER_TÉCNICO</th>\n",
       "      <th>COLE_CARACTER_TÉCNICO/ACADÉMICO</th>\n",
       "      <th>COLE_GENERO_MASCULINO</th>\n",
       "      <th>COLE_GENERO_MIXTO</th>\n",
       "      <th>...</th>\n",
       "      <th>FAMI_EDUCACIONPADRE_Secundaria (Bachillerato) incompleta</th>\n",
       "      <th>FAMI_EDUCACIONPADRE_Técnica o tecnológica completa</th>\n",
       "      <th>FAMI_EDUCACIONPADRE_Técnica o tecnológica incompleta</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 2</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 3</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 4</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 5</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 6</th>\n",
       "      <th>FAMI_TIENECOMPUTADOR_Si</th>\n",
       "      <th>FAMI_TIENEINTERNET_Si</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>281.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>307.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34770</th>\n",
       "      <td>307.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34771</th>\n",
       "      <td>288.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34772</th>\n",
       "      <td>217.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34773</th>\n",
       "      <td>244.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34774</th>\n",
       "      <td>180.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34775 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PUNT_GLOBAL  COLE_AREA_UBICACION_URBANO  COLE_BILINGUE_S  \\\n",
       "0            242.0                           0                0   \n",
       "1            272.0                           0                0   \n",
       "2            281.0                           1                0   \n",
       "3            307.0                           1                0   \n",
       "4            248.0                           1                0   \n",
       "...            ...                         ...              ...   \n",
       "34770        307.0                           1                0   \n",
       "34771        288.0                           1                0   \n",
       "34772        217.0                           1                0   \n",
       "34773        244.0                           1                0   \n",
       "34774        180.0                           1                0   \n",
       "\n",
       "       COLE_CALENDARIO_B  COLE_CALENDARIO_OTRO  COLE_CARACTER_NO APLICA  \\\n",
       "0                      0                     0                        0   \n",
       "1                      0                     0                        0   \n",
       "2                      0                     0                        0   \n",
       "3                      0                     0                        0   \n",
       "4                      0                     0                        0   \n",
       "...                  ...                   ...                      ...   \n",
       "34770                  0                     0                        0   \n",
       "34771                  0                     0                        0   \n",
       "34772                  0                     0                        0   \n",
       "34773                  0                     0                        0   \n",
       "34774                  0                     0                        0   \n",
       "\n",
       "       COLE_CARACTER_TÉCNICO  COLE_CARACTER_TÉCNICO/ACADÉMICO  \\\n",
       "0                          0                                0   \n",
       "1                          0                                0   \n",
       "2                          0                                0   \n",
       "3                          1                                0   \n",
       "4                          0                                0   \n",
       "...                      ...                              ...   \n",
       "34770                      0                                0   \n",
       "34771                      0                                0   \n",
       "34772                      0                                0   \n",
       "34773                      0                                0   \n",
       "34774                      0                                1   \n",
       "\n",
       "       COLE_GENERO_MASCULINO  COLE_GENERO_MIXTO  ...  \\\n",
       "0                          0                  1  ...   \n",
       "1                          0                  1  ...   \n",
       "2                          0                  1  ...   \n",
       "3                          0                  1  ...   \n",
       "4                          0                  1  ...   \n",
       "...                      ...                ...  ...   \n",
       "34770                      0                  1  ...   \n",
       "34771                      0                  1  ...   \n",
       "34772                      0                  1  ...   \n",
       "34773                      0                  1  ...   \n",
       "34774                      0                  1  ...   \n",
       "\n",
       "       FAMI_EDUCACIONPADRE_Secundaria (Bachillerato) incompleta  \\\n",
       "0                                                      0          \n",
       "1                                                      0          \n",
       "2                                                      0          \n",
       "3                                                      0          \n",
       "4                                                      0          \n",
       "...                                                  ...          \n",
       "34770                                                  0          \n",
       "34771                                                  0          \n",
       "34772                                                  0          \n",
       "34773                                                  0          \n",
       "34774                                                  0          \n",
       "\n",
       "       FAMI_EDUCACIONPADRE_Técnica o tecnológica completa  \\\n",
       "0                                                      0    \n",
       "1                                                      0    \n",
       "2                                                      0    \n",
       "3                                                      0    \n",
       "4                                                      0    \n",
       "...                                                  ...    \n",
       "34770                                                  0    \n",
       "34771                                                  0    \n",
       "34772                                                  0    \n",
       "34773                                                  0    \n",
       "34774                                                  0    \n",
       "\n",
       "       FAMI_EDUCACIONPADRE_Técnica o tecnológica incompleta  \\\n",
       "0                                                      0      \n",
       "1                                                      0      \n",
       "2                                                      0      \n",
       "3                                                      0      \n",
       "4                                                      0      \n",
       "...                                                  ...      \n",
       "34770                                                  0      \n",
       "34771                                                  0      \n",
       "34772                                                  0      \n",
       "34773                                                  0      \n",
       "34774                                                  0      \n",
       "\n",
       "       FAMI_ESTRATOVIVIENDA_Estrato 2  FAMI_ESTRATOVIVIENDA_Estrato 3  \\\n",
       "0                                   1                               0   \n",
       "1                                   0                               0   \n",
       "2                                   1                               0   \n",
       "3                                   0                               0   \n",
       "4                                   0                               0   \n",
       "...                               ...                             ...   \n",
       "34770                               1                               0   \n",
       "34771                               0                               0   \n",
       "34772                               1                               0   \n",
       "34773                               0                               0   \n",
       "34774                               0                               0   \n",
       "\n",
       "       FAMI_ESTRATOVIVIENDA_Estrato 4  FAMI_ESTRATOVIVIENDA_Estrato 5  \\\n",
       "0                                   0                               0   \n",
       "1                                   0                               0   \n",
       "2                                   0                               0   \n",
       "3                                   0                               0   \n",
       "4                                   0                               0   \n",
       "...                               ...                             ...   \n",
       "34770                               0                               0   \n",
       "34771                               0                               0   \n",
       "34772                               0                               0   \n",
       "34773                               0                               0   \n",
       "34774                               0                               0   \n",
       "\n",
       "       FAMI_ESTRATOVIVIENDA_Estrato 6  FAMI_TIENECOMPUTADOR_Si  \\\n",
       "0                                   0                        1   \n",
       "1                                   0                        0   \n",
       "2                                   0                        1   \n",
       "3                                   0                        1   \n",
       "4                                   0                        0   \n",
       "...                               ...                      ...   \n",
       "34770                               0                        1   \n",
       "34771                               0                        1   \n",
       "34772                               0                        0   \n",
       "34773                               0                        0   \n",
       "34774                               0                        1   \n",
       "\n",
       "       FAMI_TIENEINTERNET_Si  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          1  \n",
       "4                          0  \n",
       "...                      ...  \n",
       "34770                      0  \n",
       "34771                      1  \n",
       "34772                      0  \n",
       "34773                      0  \n",
       "34774                      0  \n",
       "\n",
       "[34775 rows x 89 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = '/Users/gabrielacorrea/Music/Proyecto 3/data_dummies.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PUNT_GLOBAL</th>\n",
       "      <th>COLE_AREA_UBICACION_URBANO</th>\n",
       "      <th>COLE_BILINGUE_S</th>\n",
       "      <th>COLE_CALENDARIO_B</th>\n",
       "      <th>COLE_CALENDARIO_OTRO</th>\n",
       "      <th>COLE_CARACTER_NO APLICA</th>\n",
       "      <th>COLE_CARACTER_TÉCNICO</th>\n",
       "      <th>COLE_CARACTER_TÉCNICO/ACADÉMICO</th>\n",
       "      <th>COLE_GENERO_MASCULINO</th>\n",
       "      <th>COLE_GENERO_MIXTO</th>\n",
       "      <th>...</th>\n",
       "      <th>FAMI_EDUCACIONPADRE_Secundaria (Bachillerato) incompleta</th>\n",
       "      <th>FAMI_EDUCACIONPADRE_Técnica o tecnológica completa</th>\n",
       "      <th>FAMI_EDUCACIONPADRE_Técnica o tecnológica incompleta</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 2</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 3</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 4</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 5</th>\n",
       "      <th>FAMI_ESTRATOVIVIENDA_Estrato 6</th>\n",
       "      <th>FAMI_TIENECOMPUTADOR_Si</th>\n",
       "      <th>FAMI_TIENEINTERNET_Si</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>217.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>202.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>272.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>286.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>257.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34745</th>\n",
       "      <td>227.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34760</th>\n",
       "      <td>189.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34765</th>\n",
       "      <td>252.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34770</th>\n",
       "      <td>307.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34774</th>\n",
       "      <td>180.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6955 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PUNT_GLOBAL  COLE_AREA_UBICACION_URBANO  COLE_BILINGUE_S  \\\n",
       "9            217.0                           1                0   \n",
       "17           202.0                           0                0   \n",
       "28           272.0                           1                0   \n",
       "39           286.0                           0                0   \n",
       "40           257.0                           1                0   \n",
       "...            ...                         ...              ...   \n",
       "34745        227.0                           1                0   \n",
       "34760        189.0                           1                1   \n",
       "34765        252.0                           1                0   \n",
       "34770        307.0                           1                0   \n",
       "34774        180.0                           1                0   \n",
       "\n",
       "       COLE_CALENDARIO_B  COLE_CALENDARIO_OTRO  COLE_CARACTER_NO APLICA  \\\n",
       "9                      0                     0                        0   \n",
       "17                     0                     0                        0   \n",
       "28                     0                     0                        0   \n",
       "39                     0                     0                        0   \n",
       "40                     0                     0                        0   \n",
       "...                  ...                   ...                      ...   \n",
       "34745                  0                     0                        0   \n",
       "34760                  0                     0                        0   \n",
       "34765                  0                     0                        0   \n",
       "34770                  0                     0                        0   \n",
       "34774                  0                     0                        0   \n",
       "\n",
       "       COLE_CARACTER_TÉCNICO  COLE_CARACTER_TÉCNICO/ACADÉMICO  \\\n",
       "9                          1                                0   \n",
       "17                         1                                0   \n",
       "28                         0                                1   \n",
       "39                         1                                0   \n",
       "40                         1                                0   \n",
       "...                      ...                              ...   \n",
       "34745                      0                                0   \n",
       "34760                      0                                0   \n",
       "34765                      0                                0   \n",
       "34770                      0                                0   \n",
       "34774                      0                                1   \n",
       "\n",
       "       COLE_GENERO_MASCULINO  COLE_GENERO_MIXTO  ...  \\\n",
       "9                          0                  1  ...   \n",
       "17                         0                  1  ...   \n",
       "28                         0                  1  ...   \n",
       "39                         0                  1  ...   \n",
       "40                         0                  0  ...   \n",
       "...                      ...                ...  ...   \n",
       "34745                      0                  1  ...   \n",
       "34760                      0                  1  ...   \n",
       "34765                      0                  1  ...   \n",
       "34770                      0                  1  ...   \n",
       "34774                      0                  1  ...   \n",
       "\n",
       "       FAMI_EDUCACIONPADRE_Secundaria (Bachillerato) incompleta  \\\n",
       "9                                                      0          \n",
       "17                                                     0          \n",
       "28                                                     0          \n",
       "39                                                     0          \n",
       "40                                                     0          \n",
       "...                                                  ...          \n",
       "34745                                                  0          \n",
       "34760                                                  0          \n",
       "34765                                                  1          \n",
       "34770                                                  0          \n",
       "34774                                                  0          \n",
       "\n",
       "       FAMI_EDUCACIONPADRE_Técnica o tecnológica completa  \\\n",
       "9                                                      0    \n",
       "17                                                     0    \n",
       "28                                                     0    \n",
       "39                                                     0    \n",
       "40                                                     0    \n",
       "...                                                  ...    \n",
       "34745                                                  0    \n",
       "34760                                                  0    \n",
       "34765                                                  0    \n",
       "34770                                                  0    \n",
       "34774                                                  0    \n",
       "\n",
       "       FAMI_EDUCACIONPADRE_Técnica o tecnológica incompleta  \\\n",
       "9                                                      0      \n",
       "17                                                     0      \n",
       "28                                                     0      \n",
       "39                                                     0      \n",
       "40                                                     0      \n",
       "...                                                  ...      \n",
       "34745                                                  0      \n",
       "34760                                                  0      \n",
       "34765                                                  0      \n",
       "34770                                                  0      \n",
       "34774                                                  0      \n",
       "\n",
       "       FAMI_ESTRATOVIVIENDA_Estrato 2  FAMI_ESTRATOVIVIENDA_Estrato 3  \\\n",
       "9                                   1                               0   \n",
       "17                                  0                               0   \n",
       "28                                  0                               1   \n",
       "39                                  1                               0   \n",
       "40                                  1                               0   \n",
       "...                               ...                             ...   \n",
       "34745                               0                               0   \n",
       "34760                               0                               0   \n",
       "34765                               0                               0   \n",
       "34770                               1                               0   \n",
       "34774                               0                               0   \n",
       "\n",
       "       FAMI_ESTRATOVIVIENDA_Estrato 4  FAMI_ESTRATOVIVIENDA_Estrato 5  \\\n",
       "9                                   0                               0   \n",
       "17                                  0                               0   \n",
       "28                                  0                               0   \n",
       "39                                  0                               0   \n",
       "40                                  0                               0   \n",
       "...                               ...                             ...   \n",
       "34745                               0                               0   \n",
       "34760                               0                               0   \n",
       "34765                               0                               0   \n",
       "34770                               0                               0   \n",
       "34774                               0                               0   \n",
       "\n",
       "       FAMI_ESTRATOVIVIENDA_Estrato 6  FAMI_TIENECOMPUTADOR_Si  \\\n",
       "9                                   0                        0   \n",
       "17                                  0                        0   \n",
       "28                                  0                        1   \n",
       "39                                  0                        0   \n",
       "40                                  0                        1   \n",
       "...                               ...                      ...   \n",
       "34745                               0                        0   \n",
       "34760                               0                        0   \n",
       "34765                               0                        1   \n",
       "34770                               0                        1   \n",
       "34774                               0                        1   \n",
       "\n",
       "       FAMI_TIENEINTERNET_Si  \n",
       "9                          0  \n",
       "17                         0  \n",
       "28                         1  \n",
       "39                         0  \n",
       "40                         1  \n",
       "...                      ...  \n",
       "34745                      0  \n",
       "34760                      0  \n",
       "34765                      1  \n",
       "34770                      0  \n",
       "34774                      0  \n",
       "\n",
       "[6955 rows x 89 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.sample(frac=0.8, random_state=100)\n",
    "train\n",
    "test = df.drop(train.index)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.copy()\n",
    "test_X = test.copy()\n",
    "train_y = train_X.pop('PUNT_GLOBAL')\n",
    "test_y = test_X.pop('PUNT_GLOBAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Separar variables predictoras (X) y la variable objetivo (y)\n",
    "X= df.drop(columns='PUNT_GLOBAL')\n",
    "y = df['PUNT_GLOBAL']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.24, random_state=1)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# mean absolute error\n",
    "MAE = metrics.mean_absolute_error(test_y, y_pred)\n",
    "\n",
    "# mean squared error\n",
    "MSE = metrics.mean_squared_error(test_y, y_pred)\n",
    "\n",
    "# root mean squared error\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  26.555046641272614\n",
      "MSE:  1139.2067195716904\n",
      "RMSE:  33.7521365186219\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE: \", MAE)\n",
    "print(\"MSE: \", MSE)\n",
    "print(\"RMSE: \", RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Separar variables predictoras (X) y la variable objetivo (y)\n",
    "X= df.drop(columns='PUNT_GLOBAL')\n",
    "y = df['PUNT_GLOBAL']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.24, random_state=1)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 14:53:35.817458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(999)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - loss: 59067.7188 - mae: 239.2903 - mse: 59061.0781 - val_loss: 56282.0898 - val_mae: 233.5272 - val_mse: 56275.3828\n",
      "Epoch 2/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 49250.7852 - mae: 217.6985 - mse: 49243.9883 - val_loss: 32580.1797 - val_mae: 173.0939 - val_mse: 32572.9238\n",
      "Epoch 3/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 22455.4102 - mae: 140.1375 - mse: 22447.9141 - val_loss: 10277.0225 - val_mae: 78.3667 - val_mse: 10268.8184\n",
      "Epoch 4/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 5589.0576 - mae: 60.0454 - mse: 5580.6616 - val_loss: 6797.2061 - val_mae: 35.3120 - val_mse: 6788.3838\n",
      "Epoch 5/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 2479.1348 - mae: 37.8638 - mse: 2470.2478 - val_loss: 6477.6641 - val_mae: 30.7357 - val_mse: 6468.6689\n",
      "Epoch 6/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 2224.4014 - mae: 35.7119 - mse: 2215.3916 - val_loss: 5878.6416 - val_mae: 29.4365 - val_mse: 5869.6060\n",
      "Epoch 7/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 2096.1133 - mae: 34.9727 - mse: 2087.0710 - val_loss: 5392.8447 - val_mae: 28.6308 - val_mse: 5383.7695\n",
      "Epoch 8/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1987.6447 - mae: 34.3099 - mse: 1978.5616 - val_loss: 5133.8696 - val_mae: 28.4180 - val_mse: 5124.7739\n",
      "Epoch 9/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1863.2291 - mae: 33.5156 - mse: 1854.1274 - val_loss: 4999.6182 - val_mae: 28.1147 - val_mse: 4990.4971\n",
      "Epoch 10/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1900.5189 - mae: 33.9532 - mse: 1891.3890 - val_loss: 4919.5435 - val_mae: 28.0036 - val_mse: 4910.3975\n",
      "Epoch 11/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1857.7321 - mae: 33.7686 - mse: 1848.5825 - val_loss: 4607.3892 - val_mae: 27.8484 - val_mse: 4598.2070\n",
      "Epoch 12/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1792.4102 - mae: 33.1051 - mse: 1783.2236 - val_loss: 4333.4688 - val_mae: 27.6807 - val_mse: 4324.2734\n",
      "Epoch 13/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1681.7626 - mae: 32.2636 - mse: 1672.5574 - val_loss: 4339.6943 - val_mae: 27.6411 - val_mse: 4330.4648\n",
      "Epoch 14/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1646.1381 - mae: 31.7680 - mse: 1636.9021 - val_loss: 4162.4395 - val_mae: 27.4246 - val_mse: 4153.1909\n",
      "Epoch 15/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1738.4020 - mae: 32.9022 - mse: 1729.1458 - val_loss: 4000.8452 - val_mae: 27.3842 - val_mse: 3991.5671\n",
      "Epoch 16/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1772.4412 - mae: 33.0959 - mse: 1763.1631 - val_loss: 3811.5259 - val_mae: 27.4610 - val_mse: 3802.2131\n",
      "Epoch 17/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1732.5930 - mae: 32.6929 - mse: 1723.2837 - val_loss: 3750.1541 - val_mae: 27.4683 - val_mse: 3740.8169\n",
      "Epoch 18/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1775.4840 - mae: 33.0188 - mse: 1766.1488 - val_loss: 3605.7234 - val_mae: 27.1265 - val_mse: 3596.3628\n",
      "Epoch 19/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1635.1132 - mae: 31.6793 - mse: 1625.7540 - val_loss: 3533.1672 - val_mae: 27.0927 - val_mse: 3523.7954\n",
      "Epoch 20/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1615.8571 - mae: 31.4461 - mse: 1606.4775 - val_loss: 3354.4626 - val_mae: 26.9979 - val_mse: 3345.0735\n",
      "Epoch 21/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1701.0809 - mae: 32.5475 - mse: 1691.6791 - val_loss: 3469.0027 - val_mae: 27.1160 - val_mse: 3459.5786\n",
      "Epoch 22/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1558.0376 - mae: 30.8835 - mse: 1548.6116 - val_loss: 3400.5203 - val_mae: 27.0553 - val_mse: 3391.0820\n",
      "Epoch 23/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1611.6925 - mae: 31.6885 - mse: 1602.2523 - val_loss: 3182.7820 - val_mae: 26.9329 - val_mse: 3173.3237\n",
      "Epoch 24/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1551.1877 - mae: 30.8902 - mse: 1541.7289 - val_loss: 3071.5996 - val_mae: 26.9102 - val_mse: 3062.1213\n",
      "Epoch 25/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1651.8584 - mae: 31.9098 - mse: 1642.3756 - val_loss: 3048.8113 - val_mae: 26.8616 - val_mse: 3039.3306\n",
      "Epoch 26/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 1587.5028 - mae: 31.4295 - mse: 1578.0111 - val_loss: 3043.7124 - val_mae: 26.9343 - val_mse: 3034.2180\n",
      "Epoch 27/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 1533.6207 - mae: 30.8305 - mse: 1524.1184 - val_loss: 2807.0088 - val_mae: 26.9413 - val_mse: 2797.4756\n",
      "Epoch 28/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 1580.2908 - mae: 31.2153 - mse: 1570.7480 - val_loss: 2922.2556 - val_mae: 26.8394 - val_mse: 2912.7087\n",
      "Epoch 29/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 1644.7041 - mae: 31.9292 - mse: 1635.1573 - val_loss: 2861.2297 - val_mae: 26.8512 - val_mse: 2851.6704\n",
      "Epoch 30/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1516.1372 - mae: 30.7198 - mse: 1506.5692 - val_loss: 2679.9934 - val_mae: 26.6952 - val_mse: 2670.4172\n",
      "Epoch 31/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 1638.8527 - mae: 31.8430 - mse: 1629.2688 - val_loss: 2585.0071 - val_mae: 26.7565 - val_mse: 2575.4121\n",
      "Epoch 32/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1630.5063 - mae: 31.8664 - mse: 1620.9050 - val_loss: 2604.9683 - val_mae: 26.9743 - val_mse: 2595.3413\n",
      "Epoch 33/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 1538.0200 - mae: 30.9016 - mse: 1528.3933 - val_loss: 2596.2708 - val_mae: 26.7622 - val_mse: 2586.6609\n",
      "Epoch 34/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1759.9731 - mae: 33.1100 - mse: 1750.3448 - val_loss: 2737.0503 - val_mae: 26.8852 - val_mse: 2727.4092\n",
      "Epoch 35/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 1430.1172 - mae: 29.7924 - mse: 1420.4747 - val_loss: 2652.8481 - val_mae: 26.7615 - val_mse: 2643.1912\n",
      "Epoch 36/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1592.0754 - mae: 31.3335 - mse: 1582.4159 - val_loss: 2611.1667 - val_mae: 26.6473 - val_mse: 2601.4934\n",
      "Epoch 37/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1453.3881 - mae: 30.0929 - mse: 1443.7151 - val_loss: 2673.9526 - val_mae: 26.7378 - val_mse: 2664.2732\n",
      "Epoch 38/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1516.9622 - mae: 30.5802 - mse: 1507.2759 - val_loss: 2599.1189 - val_mae: 26.7067 - val_mse: 2589.4275\n",
      "Epoch 39/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1508.2856 - mae: 30.4928 - mse: 1498.5908 - val_loss: 2581.9470 - val_mae: 26.7177 - val_mse: 2572.2356\n",
      "Epoch 40/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1532.9224 - mae: 30.8295 - mse: 1523.2139 - val_loss: 2533.4387 - val_mae: 26.7557 - val_mse: 2523.7039\n",
      "Epoch 41/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 1502.0582 - mae: 30.6022 - mse: 1492.3257 - val_loss: 2559.5061 - val_mae: 26.8042 - val_mse: 2549.7556\n",
      "Epoch 42/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1532.8807 - mae: 30.8201 - mse: 1523.1268 - val_loss: 2647.5850 - val_mae: 26.9159 - val_mse: 2637.8313\n",
      "Epoch 43/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1476.5435 - mae: 30.2576 - mse: 1466.7916 - val_loss: 2457.8350 - val_mae: 26.7523 - val_mse: 2448.0554\n",
      "Epoch 44/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1470.7604 - mae: 30.1311 - mse: 1460.9767 - val_loss: 2586.2996 - val_mae: 26.8469 - val_mse: 2576.5266\n",
      "Epoch 45/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1501.2450 - mae: 30.5222 - mse: 1491.4589 - val_loss: 2428.7698 - val_mae: 26.6461 - val_mse: 2418.9783\n",
      "Epoch 46/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1464.7271 - mae: 30.1289 - mse: 1454.9243 - val_loss: 2463.6912 - val_mae: 26.7137 - val_mse: 2453.8782\n",
      "Epoch 47/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1552.0583 - mae: 31.0234 - mse: 1542.2421 - val_loss: 2558.8252 - val_mae: 26.7960 - val_mse: 2549.0090\n",
      "Epoch 48/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1416.4635 - mae: 29.4755 - mse: 1406.6346 - val_loss: 2560.8997 - val_mae: 26.7919 - val_mse: 2551.0706\n",
      "Epoch 49/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 1550.2794 - mae: 30.9791 - mse: 1540.4321 - val_loss: 2662.9263 - val_mae: 26.9537 - val_mse: 2653.0818\n",
      "Epoch 50/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 1401.1195 - mae: 29.4941 - mse: 1391.2643 - val_loss: 2527.0400 - val_mae: 26.7200 - val_mse: 2517.1743\n",
      "Epoch 51/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1511.8723 - mae: 30.7025 - mse: 1501.9963 - val_loss: 2417.3757 - val_mae: 26.7388 - val_mse: 2407.4741\n",
      "Epoch 52/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1457.7454 - mae: 30.0057 - mse: 1447.8459 - val_loss: 2476.2734 - val_mae: 26.6862 - val_mse: 2466.3726\n",
      "Epoch 53/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1484.9166 - mae: 30.4603 - mse: 1475.0057 - val_loss: 2461.5801 - val_mae: 26.6985 - val_mse: 2451.6643\n",
      "Epoch 54/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 1429.3309 - mae: 29.7726 - mse: 1419.4045 - val_loss: 2564.0171 - val_mae: 26.8320 - val_mse: 2554.0830\n",
      "Epoch 55/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 1408.2522 - mae: 29.4085 - mse: 1398.3098 - val_loss: 2565.0532 - val_mae: 26.7950 - val_mse: 2555.0955\n",
      "Epoch 56/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 1495.4897 - mae: 30.4120 - mse: 1485.5222 - val_loss: 2519.1558 - val_mae: 26.7736 - val_mse: 2509.1726\n",
      "Epoch 57/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 1460.0759 - mae: 29.8300 - mse: 1450.0859 - val_loss: 2504.7488 - val_mae: 26.7774 - val_mse: 2494.7510\n",
      "Epoch 58/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1467.9165 - mae: 30.1995 - mse: 1457.9114 - val_loss: 2432.0515 - val_mae: 26.7853 - val_mse: 2422.0212\n",
      "Epoch 59/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1416.5679 - mae: 29.5872 - mse: 1406.5463 - val_loss: 2562.3953 - val_mae: 26.8508 - val_mse: 2552.3667\n",
      "Epoch 60/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1427.2262 - mae: 29.7575 - mse: 1417.1946 - val_loss: 2446.4148 - val_mae: 26.6404 - val_mse: 2436.3645\n",
      "Epoch 61/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1403.1967 - mae: 29.4294 - mse: 1393.1451 - val_loss: 2617.4717 - val_mae: 26.8906 - val_mse: 2607.4197\n",
      "Epoch 62/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1442.0627 - mae: 29.8550 - mse: 1432.0040 - val_loss: 2417.2554 - val_mae: 26.7622 - val_mse: 2407.1633\n",
      "Epoch 63/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 1428.8881 - mae: 29.8248 - mse: 1418.7953 - val_loss: 2612.8123 - val_mae: 26.8827 - val_mse: 2602.7087\n",
      "Epoch 64/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 1444.8889 - mae: 29.9034 - mse: 1434.7900 - val_loss: 2638.9946 - val_mae: 26.8619 - val_mse: 2628.8682\n",
      "Epoch 65/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 1388.4011 - mae: 29.3207 - mse: 1378.2777 - val_loss: 2542.9150 - val_mae: 26.9287 - val_mse: 2532.7495\n",
      "Epoch 66/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1443.9589 - mae: 29.8953 - mse: 1433.8026 - val_loss: 2596.4036 - val_mae: 26.8598 - val_mse: 2586.2478\n",
      "Epoch 67/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 1449.9752 - mae: 29.7783 - mse: 1439.8053 - val_loss: 2466.4255 - val_mae: 26.7835 - val_mse: 2456.2295\n",
      "Epoch 68/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 1448.8854 - mae: 30.0855 - mse: 1438.6866 - val_loss: 2469.7092 - val_mae: 26.8092 - val_mse: 2459.4912\n",
      "Epoch 69/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1424.2374 - mae: 29.7399 - mse: 1414.0187 - val_loss: 2394.7139 - val_mae: 26.6317 - val_mse: 2384.5022\n",
      "Epoch 70/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1423.6581 - mae: 29.6775 - mse: 1413.4329 - val_loss: 2554.8962 - val_mae: 26.9234 - val_mse: 2544.6423\n",
      "Epoch 71/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1401.6156 - mae: 29.4168 - mse: 1391.3655 - val_loss: 2432.8950 - val_mae: 26.7307 - val_mse: 2422.6123\n",
      "Epoch 72/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1427.8855 - mae: 29.6613 - mse: 1417.6018 - val_loss: 2444.9250 - val_mae: 26.8016 - val_mse: 2434.6526\n",
      "Epoch 73/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1412.9044 - mae: 29.5043 - mse: 1402.6154 - val_loss: 2434.4067 - val_mae: 26.9563 - val_mse: 2424.0583\n",
      "Epoch 74/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1413.1653 - mae: 29.6022 - mse: 1402.8290 - val_loss: 2439.2036 - val_mae: 26.7483 - val_mse: 2428.8601\n",
      "Epoch 75/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1378.6627 - mae: 29.1200 - mse: 1368.3204 - val_loss: 2462.8572 - val_mae: 26.7372 - val_mse: 2452.5000\n",
      "Epoch 76/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1383.4407 - mae: 29.2072 - mse: 1373.0822 - val_loss: 2572.3557 - val_mae: 26.8023 - val_mse: 2562.0005\n",
      "Epoch 77/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1411.6766 - mae: 29.6911 - mse: 1401.3080 - val_loss: 2527.5542 - val_mae: 26.8199 - val_mse: 2517.1799\n",
      "Epoch 78/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1384.2644 - mae: 29.1935 - mse: 1373.8801 - val_loss: 2568.3740 - val_mae: 26.9353 - val_mse: 2557.9575\n",
      "Epoch 79/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1382.7524 - mae: 29.2366 - mse: 1372.3424 - val_loss: 2670.2424 - val_mae: 26.9508 - val_mse: 2659.8184\n",
      "Epoch 80/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1485.2421 - mae: 30.2977 - mse: 1474.8041 - val_loss: 2498.4509 - val_mae: 26.7546 - val_mse: 2488.0054\n",
      "Epoch 81/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1382.7728 - mae: 29.2970 - mse: 1372.3138 - val_loss: 2619.0674 - val_mae: 26.8390 - val_mse: 2608.5891\n",
      "Epoch 82/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1395.0090 - mae: 29.3700 - mse: 1384.5254 - val_loss: 2528.1970 - val_mae: 26.8230 - val_mse: 2517.7114\n",
      "Epoch 83/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - loss: 1375.7145 - mae: 29.1760 - mse: 1365.2179 - val_loss: 2639.2703 - val_mae: 26.9043 - val_mse: 2628.7622\n",
      "Epoch 84/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 1325.7784 - mae: 28.5431 - mse: 1315.2704 - val_loss: 2633.4419 - val_mae: 26.9589 - val_mse: 2622.9072\n",
      "Epoch 85/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 1349.2482 - mae: 28.8396 - mse: 1338.7101 - val_loss: 2610.8005 - val_mae: 26.8468 - val_mse: 2600.2612\n",
      "Epoch 86/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 1310.2290 - mae: 28.4033 - mse: 1299.6929 - val_loss: 2726.5803 - val_mae: 27.2058 - val_mse: 2716.0034\n",
      "Epoch 87/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1380.8479 - mae: 29.1976 - mse: 1370.2826 - val_loss: 2609.1204 - val_mae: 26.9811 - val_mse: 2598.5706\n",
      "Epoch 88/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 1321.4304 - mae: 28.6358 - mse: 1310.8667 - val_loss: 2629.7769 - val_mae: 26.9551 - val_mse: 2619.2107\n",
      "Epoch 89/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 1319.6750 - mae: 28.4630 - mse: 1309.0935 - val_loss: 2610.5635 - val_mae: 26.9434 - val_mse: 2599.9602\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 17:30:47 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 17:31:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 17:31:08 INFO mlflow.tracking._tracking_service.client: 🏃 View run wistful-grouse-127 at: http://localhost:5000/#/experiments/581760674512976970/runs/fef8996f05b24578ad3c3d54a6326b40.\n",
      "2024/11/26 17:31:08 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/581760674512976970.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"tensorflow-regression-model\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.0003)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.4)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=100,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMBIO FUNCIONES DE ACTIVACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - loss: 58965.6172 - mae: 239.1226 - mse: 58958.9922 - val_loss: 57834.3516 - val_mae: 236.6900 - val_mse: 57827.6875\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 57717.8711 - mae: 236.6422 - mse: 57711.1562 - val_loss: 55433.9219 - val_mae: 231.7570 - val_mse: 55426.9844\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 55659.9961 - mae: 232.2763 - mse: 55652.9766 - val_loss: 54186.9141 - val_mae: 229.1178 - val_mse: 54179.7578\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 54202.5938 - mae: 229.2048 - mse: 54195.4141 - val_loss: 53189.6289 - val_mae: 226.9514 - val_mse: 53182.4023\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 53276.9922 - mae: 227.1358 - mse: 53269.7383 - val_loss: 52200.4492 - val_mae: 224.7803 - val_mse: 52193.1367\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 52478.1133 - mae: 225.3547 - mse: 52470.7852 - val_loss: 51238.0664 - val_mae: 222.6448 - val_mse: 51230.7031\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 51366.4258 - mae: 222.9715 - mse: 51359.0625 - val_loss: 50286.6484 - val_mae: 220.5100 - val_mse: 50279.2578\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 50419.2109 - mae: 220.8390 - mse: 50411.8125 - val_loss: 49340.6484 - val_mae: 218.3639 - val_mse: 49333.2266\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 49381.7539 - mae: 218.4483 - mse: 49374.3320 - val_loss: 48360.0898 - val_mae: 216.1145 - val_mse: 48352.6445\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 48442.9102 - mae: 216.3075 - mse: 48435.4531 - val_loss: 47336.0586 - val_mae: 213.7357 - val_mse: 47328.5938\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 47473.7773 - mae: 214.1425 - mse: 47466.3164 - val_loss: 46356.6719 - val_mae: 211.4305 - val_mse: 46349.1953\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 46646.6211 - mae: 211.9874 - mse: 46639.1367 - val_loss: 45399.7109 - val_mae: 209.1490 - val_mse: 45392.2227\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 45691.3125 - mae: 209.7701 - mse: 45683.8281 - val_loss: 44395.1250 - val_mae: 206.7195 - val_mse: 44387.6406\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 44633.1797 - mae: 207.3217 - mse: 44625.6953 - val_loss: 43323.0586 - val_mae: 204.0837 - val_mse: 43315.5703\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 43515.9531 - mae: 204.6250 - mse: 43508.4609 - val_loss: 42379.4922 - val_mae: 201.7336 - val_mse: 42372.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 42595.8672 - mae: 202.2544 - mse: 42588.3750 - val_loss: 41472.4023 - val_mae: 199.4431 - val_mse: 41464.9297\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 41783.7070 - mae: 200.1969 - mse: 41776.2344 - val_loss: 40445.7070 - val_mae: 196.7960 - val_mse: 40438.2578\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 40378.4219 - mae: 196.6830 - mse: 40370.9727 - val_loss: 39372.9258 - val_mae: 193.9719 - val_mse: 39365.4805\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 39465.9766 - mae: 194.2061 - mse: 39458.5391 - val_loss: 38403.8008 - val_mae: 191.3839 - val_mse: 38396.3633\n",
      "Epoch 20/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 38654.4492 - mae: 191.9204 - mse: 38647.0000 - val_loss: 37383.1055 - val_mae: 188.5909 - val_mse: 37375.6523\n",
      "Epoch 21/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 37464.2539 - mae: 188.7972 - mse: 37456.8047 - val_loss: 36291.9297 - val_mae: 185.5157 - val_mse: 36284.4844\n",
      "Epoch 22/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 36445.2734 - mae: 185.8929 - mse: 36437.8281 - val_loss: 35388.0938 - val_mae: 182.9493 - val_mse: 35380.6484\n",
      "Epoch 23/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 35347.0273 - mae: 182.8859 - mse: 35339.5898 - val_loss: 34482.2773 - val_mae: 180.3140 - val_mse: 34474.8555\n",
      "Epoch 24/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 34629.5898 - mae: 180.7013 - mse: 34622.1680 - val_loss: 33653.1680 - val_mae: 177.8668 - val_mse: 33645.7734\n",
      "Epoch 25/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 33879.2266 - mae: 178.4364 - mse: 33871.8398 - val_loss: 32858.4258 - val_mae: 175.4776 - val_mse: 32851.0547\n",
      "Epoch 26/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 33136.5508 - mae: 176.2983 - mse: 33129.1914 - val_loss: 32087.7480 - val_mae: 173.1176 - val_mse: 32080.4121\n",
      "Epoch 27/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 32425.2266 - mae: 174.0115 - mse: 32417.9023 - val_loss: 31344.8984 - val_mae: 170.8006 - val_mse: 31337.6094\n",
      "Epoch 28/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 31603.8887 - mae: 171.5069 - mse: 31596.6016 - val_loss: 30624.7109 - val_mae: 168.5124 - val_mse: 30617.4551\n",
      "Epoch 29/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 30865.1152 - mae: 169.1948 - mse: 30857.8672 - val_loss: 29925.2148 - val_mae: 166.2480 - val_mse: 29918.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 30324.0762 - mae: 167.3184 - mse: 30316.8613 - val_loss: 29246.6465 - val_mae: 164.0098 - val_mse: 29239.4648\n",
      "Epoch 31/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 29507.8438 - mae: 164.6967 - mse: 29500.6758 - val_loss: 28589.5625 - val_mae: 161.8011 - val_mse: 28582.4238\n",
      "Epoch 32/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 28777.2676 - mae: 162.3053 - mse: 28770.1387 - val_loss: 27948.9824 - val_mae: 159.6070 - val_mse: 27941.8926\n",
      "Epoch 33/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 28357.0918 - mae: 160.7995 - mse: 28350.0137 - val_loss: 27326.8926 - val_mae: 157.4353 - val_mse: 27319.8555\n",
      "Epoch 34/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 27432.8203 - mae: 157.8326 - mse: 27425.7969 - val_loss: 26721.0312 - val_mae: 155.2786 - val_mse: 26714.0430\n",
      "Epoch 35/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 26954.1699 - mae: 155.9413 - mse: 26947.1953 - val_loss: 26132.6797 - val_mae: 153.1426 - val_mse: 26125.7520\n",
      "Epoch 36/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 26363.9082 - mae: 153.8456 - mse: 26356.9883 - val_loss: 25560.2031 - val_mae: 151.0223 - val_mse: 25553.3262\n",
      "Epoch 37/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 25777.3223 - mae: 151.7044 - mse: 25770.4590 - val_loss: 25002.8301 - val_mae: 148.9159 - val_mse: 24996.0098\n",
      "Epoch 38/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 25411.4453 - mae: 150.0984 - mse: 25404.6406 - val_loss: 24461.8809 - val_mae: 146.8294 - val_mse: 24455.1270\n",
      "Epoch 39/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 24754.3086 - mae: 147.7774 - mse: 24747.5664 - val_loss: 23934.7383 - val_mae: 144.7535 - val_mse: 23928.0410\n",
      "Epoch 40/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 24218.4492 - mae: 145.6419 - mse: 24211.7578 - val_loss: 23421.6875 - val_mae: 142.6906 - val_mse: 23415.0469\n",
      "Epoch 41/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 23581.7012 - mae: 143.2802 - mse: 23575.0703 - val_loss: 22922.9902 - val_mae: 140.6427 - val_mse: 22916.4102\n",
      "Epoch 42/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 23277.4629 - mae: 141.7876 - mse: 23270.8984 - val_loss: 22439.6211 - val_mae: 138.6146 - val_mse: 22433.1094\n",
      "Epoch 43/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 22686.7812 - mae: 139.3214 - mse: 22680.2910 - val_loss: 21970.3125 - val_mae: 136.6027 - val_mse: 21963.8750\n",
      "Epoch 44/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 22213.7266 - mae: 137.3879 - mse: 22207.3086 - val_loss: 21513.0859 - val_mae: 134.5995 - val_mse: 21506.7305\n",
      "Epoch 45/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 21618.3555 - mae: 134.9931 - mse: 21612.0156 - val_loss: 21068.9258 - val_mae: 132.6098 - val_mse: 21062.6445\n",
      "Epoch 46/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 21364.8516 - mae: 133.7106 - mse: 21358.5898 - val_loss: 20638.1191 - val_mae: 130.6350 - val_mse: 20631.9199\n",
      "Epoch 47/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 20944.4609 - mae: 131.7078 - mse: 20938.2812 - val_loss: 20220.8340 - val_mae: 128.6770 - val_mse: 20214.7148\n",
      "Epoch 48/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 20321.5898 - mae: 128.8842 - mse: 20315.4941 - val_loss: 19815.7930 - val_mae: 126.7307 - val_mse: 19809.7598\n",
      "Epoch 49/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 20104.2754 - mae: 127.6882 - mse: 20098.2637 - val_loss: 19422.6582 - val_mae: 124.7953 - val_mse: 19416.7090\n",
      "Epoch 50/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 19730.8086 - mae: 125.8948 - mse: 19724.8789 - val_loss: 19042.1543 - val_mae: 122.8755 - val_mse: 19036.2969\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 15:18:28 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 15:18:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 15:18:44 INFO mlflow.tracking._tracking_service.client: 🏃 View run sedate-finch-938 at: http://localhost:5000/#/experiments/387834654890089351/runs/a8b07bb9da0f4f94ab45611a11613b56.\n",
      "2024/11/26 15:18:44 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/387834654890089351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "#Cambio activacion funcion\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"tensorflow-regression-model\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=512, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=256, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=128, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.0003)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.4)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 59583.7031 - mae: 240.3859 - mse: 59577.0742 - val_loss: 57770.2070 - val_mae: 236.9510 - val_mse: 57763.5547\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 58524.8203 - mae: 238.5000 - mse: 58518.1719 - val_loss: 54474.3047 - val_mae: 229.9660 - val_mse: 54467.5898\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 55431.4023 - mae: 231.9978 - mse: 55424.6445 - val_loss: 50222.9336 - val_mae: 220.0393 - val_mse: 50216.0273\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 51015.6367 - mae: 221.8121 - mse: 51008.6914 - val_loss: 48138.1367 - val_mae: 214.9959 - val_mse: 48131.0859\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 48704.5000 - mae: 216.1699 - mse: 48697.4297 - val_loss: 46701.5742 - val_mae: 211.4288 - val_mse: 46694.4531\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 47063.1328 - mae: 212.1847 - mse: 47056.0000 - val_loss: 45479.7500 - val_mae: 208.3095 - val_mse: 45472.5742\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 45562.2188 - mae: 208.4471 - mse: 45555.0352 - val_loss: 44374.8438 - val_mae: 205.4123 - val_mse: 44367.6367\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 44240.8164 - mae: 205.0836 - mse: 44233.5898 - val_loss: 43360.0859 - val_mae: 202.6818 - val_mse: 43352.8359\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 43712.0391 - mae: 203.3587 - mse: 43704.7812 - val_loss: 42412.3086 - val_mae: 200.0660 - val_mse: 42405.0312\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 42898.6953 - mae: 201.0513 - mse: 42891.4102 - val_loss: 41516.9141 - val_mae: 197.5333 - val_mse: 41509.6172\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 41696.9414 - mae: 197.8278 - mse: 41689.6406 - val_loss: 40667.9336 - val_mae: 195.0721 - val_mse: 40660.6172\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 41130.0547 - mae: 195.9810 - mse: 41122.7383 - val_loss: 39859.5469 - val_mae: 192.6708 - val_mse: 39852.2148\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 40240.1172 - mae: 193.4997 - mse: 40232.7695 - val_loss: 39084.6367 - val_mae: 190.3124 - val_mse: 39077.2812\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - loss: 39506.4648 - mae: 191.1854 - mse: 39499.1133 - val_loss: 38344.0039 - val_mae: 188.0019 - val_mse: 38336.6367\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 38648.5469 - mae: 188.5420 - mse: 38641.1758 - val_loss: 37632.4102 - val_mae: 185.7261 - val_mse: 37625.0273\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 38035.8906 - mae: 186.4528 - mse: 38028.5078 - val_loss: 36956.1367 - val_mae: 183.5090 - val_mse: 36948.7500\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 37209.7930 - mae: 184.0221 - mse: 37202.4062 - val_loss: 36299.2578 - val_mae: 181.3000 - val_mse: 36291.8555\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 36703.7500 - mae: 182.1902 - mse: 36696.3516 - val_loss: 35674.6758 - val_mae: 179.1458 - val_mse: 35667.2617\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 35753.3242 - mae: 179.4253 - mse: 35745.9062 - val_loss: 35071.0312 - val_mae: 177.0094 - val_mse: 35063.6055\n",
      "Epoch 20/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 35481.5352 - mae: 177.8911 - mse: 35474.1133 - val_loss: 34490.9570 - val_mae: 174.9024 - val_mse: 34483.5352\n",
      "Epoch 21/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 35040.2070 - mae: 176.0752 - mse: 35032.7773 - val_loss: 33931.8477 - val_mae: 172.8180 - val_mse: 33924.4141\n",
      "Epoch 22/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 34189.4688 - mae: 173.3397 - mse: 34182.0352 - val_loss: 33397.1602 - val_mae: 170.7714 - val_mse: 33389.7266\n",
      "Epoch 23/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 34072.7031 - mae: 172.2148 - mse: 34065.2695 - val_loss: 32878.9219 - val_mae: 168.7368 - val_mse: 32871.4844\n",
      "Epoch 24/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 32982.7617 - mae: 168.9339 - mse: 32975.3281 - val_loss: 32374.2109 - val_mae: 166.7074 - val_mse: 32366.7656\n",
      "Epoch 25/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 32819.7422 - mae: 167.6259 - mse: 32812.3125 - val_loss: 31897.4414 - val_mae: 164.7320 - val_mse: 31889.9941\n",
      "Epoch 26/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 31829.8926 - mae: 164.5923 - mse: 31822.4453 - val_loss: 31421.8730 - val_mae: 162.7252 - val_mse: 31414.4199\n",
      "Epoch 27/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 31649.5801 - mae: 163.2512 - mse: 31642.1289 - val_loss: 30934.7988 - val_mae: 160.6604 - val_mse: 30927.3223\n",
      "Epoch 28/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 31478.0254 - mae: 161.8080 - mse: 31470.5410 - val_loss: 30513.3887 - val_mae: 158.7532 - val_mse: 30505.8828\n",
      "Epoch 29/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 30887.3867 - mae: 159.6769 - mse: 30879.8789 - val_loss: 30108.6367 - val_mae: 156.8662 - val_mse: 30101.1328\n",
      "Epoch 30/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 30558.7441 - mae: 157.9606 - mse: 30551.2383 - val_loss: 29721.3281 - val_mae: 155.0048 - val_mse: 29713.8223\n",
      "Epoch 31/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 30449.0117 - mae: 156.6436 - mse: 30441.5078 - val_loss: 29350.1270 - val_mae: 153.1647 - val_mse: 29342.6230\n",
      "Epoch 32/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 29956.2695 - mae: 154.3170 - mse: 29948.7656 - val_loss: 28996.8027 - val_mae: 151.3576 - val_mse: 28989.3008\n",
      "Epoch 33/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 29179.6602 - mae: 151.5984 - mse: 29172.1602 - val_loss: 28619.1699 - val_mae: 149.4653 - val_mse: 28611.6562\n",
      "Epoch 34/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 29193.3105 - mae: 150.7481 - mse: 29185.8066 - val_loss: 28293.8477 - val_mae: 147.6918 - val_mse: 28286.3496\n",
      "Epoch 35/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 29033.6484 - mae: 149.3400 - mse: 29026.1504 - val_loss: 27984.7715 - val_mae: 145.9502 - val_mse: 27977.2734\n",
      "Epoch 36/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 28448.1758 - mae: 147.0316 - mse: 28440.6797 - val_loss: 27689.2773 - val_mae: 144.2290 - val_mse: 27681.7852\n",
      "Epoch 37/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 28001.6484 - mae: 144.7921 - mse: 27994.1582 - val_loss: 27406.8223 - val_mae: 142.5257 - val_mse: 27399.3340\n",
      "Epoch 38/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 27644.1562 - mae: 142.9963 - mse: 27636.6660 - val_loss: 27138.8477 - val_mae: 140.8522 - val_mse: 27131.3574\n",
      "Epoch 39/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 27801.3184 - mae: 142.3915 - mse: 27793.8398 - val_loss: 26882.2520 - val_mae: 139.1913 - val_mse: 26874.7734\n",
      "Epoch 40/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 27514.7148 - mae: 140.5397 - mse: 27507.2402 - val_loss: 26640.4609 - val_mae: 137.5688 - val_mse: 26632.9902\n",
      "Epoch 41/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 27376.8750 - mae: 139.0724 - mse: 27369.4121 - val_loss: 26411.9043 - val_mae: 135.9778 - val_mse: 26404.4434\n",
      "Epoch 42/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 26820.3438 - mae: 136.5677 - mse: 26812.8848 - val_loss: 26193.6406 - val_mae: 134.3990 - val_mse: 26186.1914\n",
      "Epoch 43/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 26955.3613 - mae: 136.1520 - mse: 26947.9121 - val_loss: 25986.4375 - val_mae: 132.8401 - val_mse: 25979.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 26470.4902 - mae: 133.7275 - mse: 26463.0566 - val_loss: 25791.5332 - val_mae: 131.3140 - val_mse: 25784.1113\n",
      "Epoch 45/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 26755.3281 - mae: 133.3962 - mse: 26747.9004 - val_loss: 25609.1133 - val_mae: 129.8268 - val_mse: 25601.7012\n",
      "Epoch 46/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26154.5859 - mae: 131.1678 - mse: 26147.1777 - val_loss: 25434.3496 - val_mae: 128.3403 - val_mse: 25426.9512\n",
      "Epoch 47/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 26776.6250 - mae: 131.3169 - mse: 26769.2285 - val_loss: 25273.5371 - val_mae: 126.9144 - val_mse: 25266.1484\n",
      "Epoch 48/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 25712.5156 - mae: 128.0349 - mse: 25705.1270 - val_loss: 25119.4512 - val_mae: 125.4863 - val_mse: 25112.0742\n",
      "Epoch 49/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 25848.4082 - mae: 126.9823 - mse: 25841.0312 - val_loss: 24977.5586 - val_mae: 124.1153 - val_mse: 24970.1992\n",
      "Epoch 50/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 25793.6914 - mae: 126.3333 - mse: 25786.3320 - val_loss: 24844.3398 - val_mae: 122.7695 - val_mse: 24836.9980\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 15:21:07 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 15:21:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 15:21:24 INFO mlflow.tracking._tracking_service.client: 🏃 View run Modelo tahn at: http://localhost:5000/#/experiments/387834654890089351/runs/39be512bd650482c8bc70817bc963f91.\n",
      "2024/11/26 15:21:24 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/387834654890089351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "#Cambio activacion funcion\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"tensorflow-regression-model\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=512, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=256, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=128, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.0003)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.4)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 59492.7773 - mae: 240.1580 - mse: 59487.1406 - val_loss: 59259.0039 - val_mae: 239.6403 - val_mse: 59255.1680\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 59562.5977 - mae: 240.3628 - mse: 59558.9023 - val_loss: 59188.6680 - val_mae: 239.4939 - val_mse: 59185.0039\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 59501.3008 - mae: 240.1172 - mse: 59497.5781 - val_loss: 59134.3242 - val_mae: 239.3800 - val_mse: 59130.4609\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 59506.1250 - mae: 240.1455 - mse: 59502.2656 - val_loss: 59090.1484 - val_mae: 239.2879 - val_mse: 59086.3594\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 59594.7227 - mae: 240.3092 - mse: 59590.9922 - val_loss: 59036.5234 - val_mae: 239.1766 - val_mse: 59033.1055\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 59330.3438 - mae: 239.7608 - mse: 59327.0547 - val_loss: 58987.4531 - val_mae: 239.0752 - val_mse: 58984.5352\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 59383.6523 - mae: 239.9054 - mse: 59380.8359 - val_loss: 58951.8438 - val_mae: 239.0015 - val_mse: 58949.3281\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 59392.5352 - mae: 239.9040 - mse: 59390.1055 - val_loss: 58920.5195 - val_mae: 238.9366 - val_mse: 58918.3398\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 59266.9375 - mae: 239.6384 - mse: 59264.8281 - val_loss: 58891.2617 - val_mae: 238.8761 - val_mse: 58889.3633\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 58802.9961 - mae: 238.6680 - mse: 58801.1641 - val_loss: 58863.2422 - val_mae: 238.8180 - val_mse: 58861.6016\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 59051.1445 - mae: 239.2402 - mse: 59049.5469 - val_loss: 58836.0664 - val_mae: 238.7615 - val_mse: 58834.6328\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 59040.1094 - mae: 239.1520 - mse: 59038.7109 - val_loss: 58809.4844 - val_mae: 238.7063 - val_mse: 58808.2344\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 59173.4570 - mae: 239.3736 - mse: 59172.2305 - val_loss: 58783.3828 - val_mae: 238.6519 - val_mse: 58782.2617\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 59217.0586 - mae: 239.5124 - mse: 59215.9688 - val_loss: 58757.6250 - val_mae: 238.5982 - val_mse: 58756.6445\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - loss: 59099.3047 - mae: 239.2872 - mse: 59098.3320 - val_loss: 58732.1602 - val_mae: 238.5451 - val_mse: 58731.2734\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 58901.4023 - mae: 238.8499 - mse: 58900.5352 - val_loss: 58706.9531 - val_mae: 238.4924 - val_mse: 58706.1445\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 58742.7188 - mae: 238.5888 - mse: 58741.9336 - val_loss: 58681.9023 - val_mae: 238.4401 - val_mse: 58681.1719\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 59027.8359 - mae: 239.1685 - mse: 59027.1211 - val_loss: 58657.0469 - val_mae: 238.3880 - val_mse: 58656.3828\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 58976.2812 - mae: 238.9669 - mse: 58975.6445 - val_loss: 58632.3242 - val_mae: 238.3363 - val_mse: 58631.7109\n",
      "Epoch 20/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 59117.4844 - mae: 239.2797 - mse: 59116.8828 - val_loss: 58607.6992 - val_mae: 238.2848 - val_mse: 58607.1328\n",
      "Epoch 21/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 59093.7227 - mae: 239.2146 - mse: 59093.1523 - val_loss: 58583.2148 - val_mae: 238.2335 - val_mse: 58582.6836\n",
      "Epoch 22/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 58754.6016 - mae: 238.5571 - mse: 58754.0742 - val_loss: 58558.8203 - val_mae: 238.1824 - val_mse: 58558.3125\n",
      "Epoch 23/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 58966.1406 - mae: 238.9868 - mse: 58965.6406 - val_loss: 58534.4883 - val_mae: 238.1313 - val_mse: 58534.0156\n",
      "Epoch 24/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 58942.5352 - mae: 238.9787 - mse: 58942.0547 - val_loss: 58510.2500 - val_mae: 238.0805 - val_mse: 58509.7852\n",
      "Epoch 25/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 59007.0352 - mae: 239.0702 - mse: 59006.5898 - val_loss: 58486.0547 - val_mae: 238.0298 - val_mse: 58485.6172\n",
      "Epoch 26/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 58794.8203 - mae: 238.6679 - mse: 58794.3789 - val_loss: 58461.9297 - val_mae: 237.9792 - val_mse: 58461.5078\n",
      "Epoch 27/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 58811.5352 - mae: 238.7415 - mse: 58811.1172 - val_loss: 58437.8438 - val_mae: 237.9286 - val_mse: 58437.4414\n",
      "Epoch 28/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 58691.0664 - mae: 238.3944 - mse: 58690.6602 - val_loss: 58413.8164 - val_mae: 237.8781 - val_mse: 58413.4141\n",
      "Epoch 29/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 58674.5078 - mae: 238.4426 - mse: 58674.1172 - val_loss: 58389.8164 - val_mae: 237.8277 - val_mse: 58389.4297\n",
      "Epoch 30/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 58670.0742 - mae: 238.3682 - mse: 58669.6953 - val_loss: 58365.8516 - val_mae: 237.7774 - val_mse: 58365.4805\n",
      "Epoch 31/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 58651.4141 - mae: 238.3286 - mse: 58651.0469 - val_loss: 58341.9219 - val_mae: 237.7271 - val_mse: 58341.5703\n",
      "Epoch 32/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 58951.5039 - mae: 238.9483 - mse: 58951.1367 - val_loss: 58318.0273 - val_mae: 237.6769 - val_mse: 58317.6797\n",
      "Epoch 33/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 58342.7227 - mae: 237.7313 - mse: 58342.3633 - val_loss: 58294.1758 - val_mae: 237.6268 - val_mse: 58293.8281\n",
      "Epoch 34/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 58370.0469 - mae: 237.8363 - mse: 58369.6875 - val_loss: 58270.3477 - val_mae: 237.5766 - val_mse: 58270.0078\n",
      "Epoch 35/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 58453.1016 - mae: 237.9517 - mse: 58452.7656 - val_loss: 58246.5430 - val_mae: 237.5265 - val_mse: 58246.2031\n",
      "Epoch 36/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 58249.7070 - mae: 237.5121 - mse: 58249.3711 - val_loss: 58222.7461 - val_mae: 237.4765 - val_mse: 58222.4336\n",
      "Epoch 37/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 58587.4453 - mae: 238.2131 - mse: 58587.1250 - val_loss: 58198.9609 - val_mae: 237.4265 - val_mse: 58198.6406\n",
      "Epoch 38/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 58465.1758 - mae: 237.9643 - mse: 58464.8672 - val_loss: 58175.2266 - val_mae: 237.3765 - val_mse: 58174.9180\n",
      "Epoch 39/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 58665.8477 - mae: 238.3695 - mse: 58665.5352 - val_loss: 58151.5000 - val_mae: 237.3265 - val_mse: 58151.1914\n",
      "Epoch 40/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 58396.4766 - mae: 237.8031 - mse: 58396.1641 - val_loss: 58127.7773 - val_mae: 237.2766 - val_mse: 58127.4805\n",
      "Epoch 41/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 58553.7773 - mae: 238.1486 - mse: 58553.4727 - val_loss: 58104.0820 - val_mae: 237.2267 - val_mse: 58103.7969\n",
      "Epoch 42/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 58570.1875 - mae: 238.2094 - mse: 58569.8945 - val_loss: 58080.4102 - val_mae: 237.1768 - val_mse: 58080.1211\n",
      "Epoch 43/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 58575.4961 - mae: 238.1886 - mse: 58575.1875 - val_loss: 58056.7344 - val_mae: 237.1268 - val_mse: 58056.4492\n",
      "Epoch 44/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 58370.4492 - mae: 237.7572 - mse: 58370.1562 - val_loss: 58033.0820 - val_mae: 237.0770 - val_mse: 58032.7930\n",
      "Epoch 45/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 58157.6680 - mae: 237.4371 - mse: 58157.3750 - val_loss: 58009.4453 - val_mae: 237.0272 - val_mse: 58009.1602\n",
      "Epoch 46/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - loss: 58381.2070 - mae: 237.7502 - mse: 58380.9219 - val_loss: 57985.8164 - val_mae: 236.9774 - val_mse: 57985.5273\n",
      "Epoch 47/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 58265.8438 - mae: 237.5562 - mse: 58265.5664 - val_loss: 57962.1875 - val_mae: 236.9276 - val_mse: 57961.9102\n",
      "Epoch 48/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 58214.6445 - mae: 237.4370 - mse: 58214.3750 - val_loss: 57938.5898 - val_mae: 236.8778 - val_mse: 57938.3242\n",
      "Epoch 49/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 58250.9414 - mae: 237.5216 - mse: 58250.6719 - val_loss: 57915.0039 - val_mae: 236.8281 - val_mse: 57914.7344\n",
      "Epoch 50/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 58439.8320 - mae: 237.9176 - mse: 58439.5625 - val_loss: 57891.4141 - val_mae: 236.7783 - val_mse: 57891.1484\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 17:16:13 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 17:16:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 17:16:34 INFO mlflow.tracking._tracking_service.client: 🏃 View run rambunctious-ape-295 at: http://localhost:5000/#/experiments/387834654890089351/runs/d8f42d7175fb4af798ffea059564b4f4.\n",
      "2024/11/26 17:16:34 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/387834654890089351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "#Cambio activacion funcion\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"tensorflow-regression-model\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=512, activation='softmax', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=256, activation='softmax', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=128, activation='softmax', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.0003)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.4)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMBIO NEURONAS DE LAS CAPAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Capa Densa 1: 256\n",
    "2. Capa Densa 2: 128\n",
    "3. Capa Densa 3: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 17:23:56 INFO mlflow.tracking.fluent: Experiment with name 'tensorflow-regression-model' does not exist. Creating a new experiment.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 59084.6406 - mae: 239.2874 - mse: 59080.7852 - val_loss: 58059.0117 - val_mae: 237.2356 - val_mse: 58055.1055\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 56375.7031 - mae: 233.8769 - mse: 56371.7617 - val_loss: 52819.5469 - val_mae: 226.3863 - val_mse: 52815.4844\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 49839.3984 - mae: 219.7583 - mse: 49835.2812 - val_loss: 37441.1094 - val_mae: 189.3648 - val_mse: 37436.7852\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 35983.7969 - mae: 184.5115 - mse: 35979.3789 - val_loss: 19433.3457 - val_mae: 130.8298 - val_mse: 19428.6523\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 20559.4688 - mae: 133.5280 - mse: 20554.6738 - val_loss: 9846.7207 - val_mae: 82.1536 - val_mse: 9841.6123\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 10848.2803 - mae: 89.9027 - mse: 10843.0732 - val_loss: 5431.2729 - val_mae: 56.8367 - val_mse: 5425.7930\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5821.4009 - mae: 62.5455 - mse: 5815.8438 - val_loss: 3370.6406 - val_mae: 39.8748 - val_mse: 3364.8748\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 3156.1145 - mae: 45.1478 - mse: 3150.2952 - val_loss: 2756.6980 - val_mae: 30.3774 - val_mse: 2750.7483\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2332.8086 - mae: 38.2135 - mse: 2326.8303 - val_loss: 2692.7307 - val_mae: 27.7164 - val_mse: 2686.6870\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2074.9463 - mae: 35.9987 - mse: 2068.8901 - val_loss: 2601.4265 - val_mae: 27.2854 - val_mse: 2595.3428\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2070.1077 - mae: 35.8138 - mse: 2064.0173 - val_loss: 2579.2200 - val_mae: 27.2361 - val_mse: 2573.1211\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2131.3306 - mae: 36.3449 - mse: 2125.2310 - val_loss: 2524.0449 - val_mae: 27.1340 - val_mse: 2517.9375\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 2042.8539 - mae: 35.7772 - mse: 2036.7443 - val_loss: 2472.8037 - val_mae: 27.0763 - val_mse: 2466.6895\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1989.3812 - mae: 35.2383 - mse: 1983.2656 - val_loss: 2427.4932 - val_mae: 27.0079 - val_mse: 2421.3748\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2042.9674 - mae: 35.6444 - mse: 2036.8489 - val_loss: 2391.3318 - val_mae: 26.9539 - val_mse: 2385.2063\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1991.5529 - mae: 35.3922 - mse: 1985.4272 - val_loss: 2338.0022 - val_mae: 26.9028 - val_mse: 2331.8701\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1928.6531 - mae: 34.8402 - mse: 1922.5192 - val_loss: 2337.9712 - val_mae: 26.8899 - val_mse: 2331.8398\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1951.3796 - mae: 34.7957 - mse: 1945.2477 - val_loss: 2332.8140 - val_mae: 26.8704 - val_mse: 2326.6704\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1986.0886 - mae: 35.2146 - mse: 1979.9458 - val_loss: 2251.6685 - val_mae: 26.7445 - val_mse: 2245.5212\n",
      "Epoch 20/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1976.2681 - mae: 34.8227 - mse: 1970.1184 - val_loss: 2296.1387 - val_mae: 26.8183 - val_mse: 2289.9895\n",
      "Epoch 21/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1926.6154 - mae: 34.6609 - mse: 1920.4652 - val_loss: 2232.6592 - val_mae: 26.7536 - val_mse: 2226.5002\n",
      "Epoch 22/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1938.8022 - mae: 34.6943 - mse: 1932.6451 - val_loss: 2226.8899 - val_mae: 26.8342 - val_mse: 2220.7324\n",
      "Epoch 23/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1884.9696 - mae: 34.1589 - mse: 1878.8085 - val_loss: 2153.4683 - val_mae: 26.7188 - val_mse: 2147.3005\n",
      "Epoch 24/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1818.1432 - mae: 33.5533 - mse: 1811.9755 - val_loss: 2164.9780 - val_mae: 26.7345 - val_mse: 2158.8101\n",
      "Epoch 25/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1888.8832 - mae: 34.1626 - mse: 1882.7125 - val_loss: 2146.6753 - val_mae: 26.7039 - val_mse: 2140.5012\n",
      "Epoch 26/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1884.9652 - mae: 34.1403 - mse: 1878.7898 - val_loss: 2109.6699 - val_mae: 26.7075 - val_mse: 2103.4907\n",
      "Epoch 27/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1836.7495 - mae: 33.8498 - mse: 1830.5717 - val_loss: 2070.3569 - val_mae: 26.6942 - val_mse: 2064.1667\n",
      "Epoch 28/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1982.1899 - mae: 35.2533 - mse: 1976.0005 - val_loss: 2093.4307 - val_mae: 26.6925 - val_mse: 2087.2498\n",
      "Epoch 29/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1833.7001 - mae: 33.7776 - mse: 1827.5145 - val_loss: 2035.2041 - val_mae: 26.5645 - val_mse: 2029.0125\n",
      "Epoch 30/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2007.2516 - mae: 35.4405 - mse: 2001.0577 - val_loss: 2026.6624 - val_mae: 26.6001 - val_mse: 2020.4694\n",
      "Epoch 31/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1831.5446 - mae: 33.8788 - mse: 1825.3490 - val_loss: 2033.7477 - val_mae: 26.6158 - val_mse: 2027.5454\n",
      "Epoch 32/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1784.3573 - mae: 33.4195 - mse: 1778.1559 - val_loss: 1976.9845 - val_mae: 26.5570 - val_mse: 1970.7781\n",
      "Epoch 33/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1832.5366 - mae: 33.8343 - mse: 1826.3293 - val_loss: 1958.5547 - val_mae: 26.6302 - val_mse: 1952.3590\n",
      "Epoch 34/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1867.9028 - mae: 34.1790 - mse: 1861.7052 - val_loss: 1945.1680 - val_mae: 26.6430 - val_mse: 1938.9587\n",
      "Epoch 35/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1787.5610 - mae: 33.4109 - mse: 1781.3512 - val_loss: 1912.1119 - val_mae: 26.5417 - val_mse: 1905.9009\n",
      "Epoch 36/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1721.6019 - mae: 32.7528 - mse: 1715.3898 - val_loss: 1911.5940 - val_mae: 26.5289 - val_mse: 1905.3806\n",
      "Epoch 37/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1776.6625 - mae: 33.3121 - mse: 1770.4474 - val_loss: 1914.9977 - val_mae: 26.5714 - val_mse: 1908.7770\n",
      "Epoch 38/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1839.4001 - mae: 33.8133 - mse: 1833.1780 - val_loss: 1885.0076 - val_mae: 26.5490 - val_mse: 1878.7894\n",
      "Epoch 39/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1862.3732 - mae: 34.1373 - mse: 1856.1519 - val_loss: 1856.7787 - val_mae: 26.6289 - val_mse: 1850.5524\n",
      "Epoch 40/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1766.3999 - mae: 33.2952 - mse: 1760.1742 - val_loss: 1855.0121 - val_mae: 26.5492 - val_mse: 1848.7866\n",
      "Epoch 41/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1746.5416 - mae: 33.0986 - mse: 1740.3163 - val_loss: 1869.3184 - val_mae: 26.5382 - val_mse: 1863.0906\n",
      "Epoch 42/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1772.4567 - mae: 33.0873 - mse: 1766.2271 - val_loss: 1875.2904 - val_mae: 26.5727 - val_mse: 1869.0581\n",
      "Epoch 43/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1755.4169 - mae: 33.0534 - mse: 1749.1832 - val_loss: 1831.8359 - val_mae: 26.5492 - val_mse: 1825.6006\n",
      "Epoch 44/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 1869.5682 - mae: 34.1861 - mse: 1863.3337 - val_loss: 1833.4807 - val_mae: 26.5257 - val_mse: 1827.2423\n",
      "Epoch 45/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1796.3481 - mae: 33.6416 - mse: 1790.1091 - val_loss: 1795.9233 - val_mae: 26.5132 - val_mse: 1789.6803\n",
      "Epoch 46/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1780.6417 - mae: 33.3355 - mse: 1774.3999 - val_loss: 1772.2145 - val_mae: 26.4955 - val_mse: 1765.9718\n",
      "Epoch 47/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1848.8115 - mae: 33.8489 - mse: 1842.5670 - val_loss: 1808.3108 - val_mae: 26.5814 - val_mse: 1802.0670\n",
      "Epoch 48/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1749.6302 - mae: 33.1368 - mse: 1743.3848 - val_loss: 1765.8927 - val_mae: 26.5184 - val_mse: 1759.6400\n",
      "Epoch 49/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1773.6984 - mae: 33.1726 - mse: 1767.4502 - val_loss: 1766.0094 - val_mae: 26.5211 - val_mse: 1759.7599\n",
      "Epoch 50/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1725.6886 - mae: 32.9328 - mse: 1719.4382 - val_loss: 1801.1346 - val_mae: 26.5386 - val_mse: 1794.8844\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 17:24:50 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 17:25:07 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 17:25:07 INFO mlflow.tracking._tracking_service.client: 🏃 View run unruly-cub-307 at: http://localhost:5000/#/experiments/581760674512976970/runs/6cf7f24f114345b1b38ef0f6960e7202.\n",
      "2024/11/26 17:25:07 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/581760674512976970.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"tensorflow-regression-model\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.0003)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.4)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Capa Densa 1: 128\n",
    "2. Capa Densa 2: 64\n",
    "3. Capa Densa 3: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 59620.2695 - mae: 240.3800 - mse: 59617.9648 - val_loss: 58857.6875 - val_mae: 238.8060 - val_mse: 58855.3633\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 58806.5898 - mae: 238.6383 - mse: 58804.2656 - val_loss: 57812.3242 - val_mae: 236.7038 - val_mse: 57809.9492\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 57619.2812 - mae: 236.3009 - mse: 57616.8906 - val_loss: 55365.3320 - val_mae: 231.7165 - val_mse: 55362.8750\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 55294.1875 - mae: 231.4904 - mse: 55291.6992 - val_loss: 50782.8906 - val_mae: 221.7295 - val_mse: 50780.3125\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 50978.3008 - mae: 222.1802 - mse: 50975.6836 - val_loss: 44294.5156 - val_mae: 206.4510 - val_mse: 44291.7734\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 44839.0703 - mae: 207.8041 - mse: 44836.2930 - val_loss: 35209.9844 - val_mae: 183.1507 - val_mse: 35207.0508\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 36447.7070 - mae: 186.2956 - mse: 36444.7305 - val_loss: 26115.6133 - val_mae: 156.6263 - val_mse: 26112.4668\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 27670.7715 - mae: 160.9523 - mse: 27667.5566 - val_loss: 18280.5332 - val_mae: 129.3587 - val_mse: 18277.1387\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 19841.2285 - mae: 133.9680 - mse: 19837.7695 - val_loss: 12063.8496 - val_mae: 102.5413 - val_mse: 12060.2061\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 13548.7168 - mae: 107.5046 - mse: 13545.0127 - val_loss: 7735.3765 - val_mae: 78.5469 - val_mse: 7731.4951\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 8742.9141 - mae: 82.5283 - mse: 8738.9795 - val_loss: 4811.1221 - val_mae: 57.1263 - val_mse: 4807.0244\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 5905.8711 - mae: 64.2604 - mse: 5901.7241 - val_loss: 3254.1267 - val_mae: 42.3503 - val_mse: 3249.8501\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 4084.3357 - mae: 51.4563 - mse: 4080.0217 - val_loss: 2444.7461 - val_mae: 33.6013 - val_mse: 2440.3320\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3181.1021 - mae: 44.5309 - mse: 3176.6636 - val_loss: 2085.6189 - val_mae: 29.6909 - val_mse: 2081.1138\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2835.5261 - mae: 42.1999 - mse: 2831.0037 - val_loss: 1948.3197 - val_mae: 28.3070 - val_mse: 1943.7563\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2773.5742 - mae: 41.6361 - mse: 2769.0022 - val_loss: 1922.8958 - val_mae: 27.7251 - val_mse: 1918.3014\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2699.2637 - mae: 41.2040 - mse: 2694.6638 - val_loss: 1901.9481 - val_mae: 27.4176 - val_mse: 1897.3368\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2603.5046 - mae: 40.5034 - mse: 2598.8894 - val_loss: 1853.6661 - val_mae: 27.2451 - val_mse: 1849.0430\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2615.0249 - mae: 40.3991 - mse: 2610.3994 - val_loss: 1837.5361 - val_mae: 27.0875 - val_mse: 1832.9065\n",
      "Epoch 20/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2577.4917 - mae: 40.0603 - mse: 2572.8606 - val_loss: 1836.5040 - val_mae: 27.1219 - val_mse: 1831.8733\n",
      "Epoch 21/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2583.6440 - mae: 40.2256 - mse: 2579.0144 - val_loss: 1813.9866 - val_mae: 27.0531 - val_mse: 1809.3561\n",
      "Epoch 22/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2626.9519 - mae: 40.6499 - mse: 2622.3232 - val_loss: 1802.8401 - val_mae: 27.0027 - val_mse: 1798.2079\n",
      "Epoch 23/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2552.7202 - mae: 39.8634 - mse: 2548.0874 - val_loss: 1768.5547 - val_mae: 26.9111 - val_mse: 1763.9209\n",
      "Epoch 24/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2529.0972 - mae: 39.8008 - mse: 2524.4636 - val_loss: 1777.0093 - val_mae: 26.9537 - val_mse: 1772.3761\n",
      "Epoch 25/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 2465.3833 - mae: 39.4753 - mse: 2460.7493 - val_loss: 1741.7688 - val_mae: 26.8974 - val_mse: 1737.1322\n",
      "Epoch 26/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2538.4355 - mae: 39.7155 - mse: 2533.7991 - val_loss: 1744.2423 - val_mae: 26.9280 - val_mse: 1739.6040\n",
      "Epoch 27/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2489.6316 - mae: 39.3879 - mse: 2484.9941 - val_loss: 1719.8771 - val_mae: 26.8446 - val_mse: 1715.2374\n",
      "Epoch 28/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2338.0918 - mae: 38.2229 - mse: 2333.4514 - val_loss: 1710.2716 - val_mae: 26.8880 - val_mse: 1705.6321\n",
      "Epoch 29/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2407.7324 - mae: 38.7875 - mse: 2403.0920 - val_loss: 1698.9994 - val_mae: 26.8276 - val_mse: 1694.3561\n",
      "Epoch 30/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 2438.9392 - mae: 38.9199 - mse: 2434.2939 - val_loss: 1682.7888 - val_mae: 26.7610 - val_mse: 1678.1422\n",
      "Epoch 31/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2328.1091 - mae: 38.2856 - mse: 2323.4600 - val_loss: 1660.6022 - val_mae: 26.7537 - val_mse: 1655.9497\n",
      "Epoch 32/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2388.0452 - mae: 38.8363 - mse: 2383.3931 - val_loss: 1644.9866 - val_mae: 26.7157 - val_mse: 1640.3351\n",
      "Epoch 33/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2416.2290 - mae: 38.8598 - mse: 2411.5779 - val_loss: 1629.8148 - val_mae: 26.7562 - val_mse: 1625.1631\n",
      "Epoch 34/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2314.3579 - mae: 38.0593 - mse: 2309.7065 - val_loss: 1617.6086 - val_mae: 26.7172 - val_mse: 1612.9546\n",
      "Epoch 35/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2281.9126 - mae: 37.6730 - mse: 2277.2590 - val_loss: 1630.5619 - val_mae: 26.6786 - val_mse: 1625.9070\n",
      "Epoch 36/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2273.8154 - mae: 37.7347 - mse: 2269.1594 - val_loss: 1616.4329 - val_mae: 26.6765 - val_mse: 1611.7737\n",
      "Epoch 37/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2275.7627 - mae: 37.9831 - mse: 2271.1028 - val_loss: 1608.3638 - val_mae: 26.6384 - val_mse: 1603.7017\n",
      "Epoch 38/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2369.3789 - mae: 38.2918 - mse: 2364.7158 - val_loss: 1589.8203 - val_mae: 26.6767 - val_mse: 1585.1537\n",
      "Epoch 39/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2222.8301 - mae: 37.3361 - mse: 2218.1631 - val_loss: 1581.1725 - val_mae: 26.5855 - val_mse: 1576.5024\n",
      "Epoch 40/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2351.6274 - mae: 38.3225 - mse: 2346.9580 - val_loss: 1548.4851 - val_mae: 26.5893 - val_mse: 1543.8132\n",
      "Epoch 41/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 2190.5325 - mae: 37.0410 - mse: 2185.8591 - val_loss: 1539.3372 - val_mae: 26.5982 - val_mse: 1534.6593\n",
      "Epoch 42/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2204.3049 - mae: 36.8935 - mse: 2199.6304 - val_loss: 1538.9607 - val_mae: 26.5761 - val_mse: 1534.2849\n",
      "Epoch 43/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 2358.0791 - mae: 38.5081 - mse: 2353.4028 - val_loss: 1528.2468 - val_mae: 26.5882 - val_mse: 1523.5699\n",
      "Epoch 44/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2218.3911 - mae: 37.3049 - mse: 2213.7126 - val_loss: 1518.6735 - val_mae: 26.5308 - val_mse: 1513.9944\n",
      "Epoch 45/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2275.9224 - mae: 37.6001 - mse: 2271.2429 - val_loss: 1498.8325 - val_mae: 26.5549 - val_mse: 1494.1494\n",
      "Epoch 46/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2304.0771 - mae: 37.9974 - mse: 2299.3933 - val_loss: 1481.0623 - val_mae: 26.5162 - val_mse: 1476.3778\n",
      "Epoch 47/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2292.5234 - mae: 37.7002 - mse: 2287.8372 - val_loss: 1478.9625 - val_mae: 26.4829 - val_mse: 1474.2754\n",
      "Epoch 48/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2407.5522 - mae: 38.5874 - mse: 2402.8647 - val_loss: 1481.2344 - val_mae: 26.5392 - val_mse: 1476.5483\n",
      "Epoch 49/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2195.9597 - mae: 36.8879 - mse: 2191.2710 - val_loss: 1462.0806 - val_mae: 26.4079 - val_mse: 1457.3894\n",
      "Epoch 50/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2155.8040 - mae: 36.6738 - mse: 2151.1130 - val_loss: 1453.7537 - val_mae: 26.4290 - val_mse: 1449.0587\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 17:26:00 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 17:26:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 17:26:20 INFO mlflow.tracking._tracking_service.client: 🏃 View run nosy-stork-713 at: http://localhost:5000/#/experiments/581760674512976970/runs/142fdacde5694fcb9726ab0756af90d5.\n",
      "2024/11/26 17:26:20 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/581760674512976970.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"tensorflow-regression-model\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.0003)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.4)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMBIO TASA DE APRENDIZAJE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso 1: La mitad de la original 0.00015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 17:38:53 INFO mlflow.tracking.fluent: Experiment with name 'tensorflow-regression-model' does not exist. Creating a new experiment.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - loss: 59095.2305 - mae: 239.3382 - mse: 59088.6133 - val_loss: 58392.6445 - val_mae: 237.8790 - val_mse: 58386.0156\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 57093.9648 - mae: 235.2161 - mse: 57087.3047 - val_loss: 54223.7109 - val_mae: 229.1951 - val_mse: 54216.9961\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 50726.7656 - mae: 221.5533 - mse: 50720.0078 - val_loss: 41235.5156 - val_mae: 198.4035 - val_mse: 41228.5781\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 38542.0000 - mae: 191.5551 - mse: 38534.9766 - val_loss: 23977.6465 - val_mae: 147.0826 - val_mse: 23970.3516\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 22898.9180 - mae: 144.3311 - mse: 22891.5117 - val_loss: 10858.1934 - val_mae: 90.1581 - val_mse: 10850.4580\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 10794.3076 - mae: 93.5128 - mse: 10786.4609 - val_loss: 5635.2036 - val_mae: 48.0052 - val_mse: 5627.0488\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4908.1987 - mae: 56.5405 - mse: 4899.9585 - val_loss: 4784.8564 - val_mae: 33.8743 - val_mse: 4776.3965\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 2742.9832 - mae: 40.0434 - mse: 2734.4746 - val_loss: 5112.8989 - val_mae: 30.7476 - val_mse: 5104.2715\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 2261.0134 - mae: 36.6318 - mse: 2252.3640 - val_loss: 5432.0024 - val_mae: 29.7669 - val_mse: 5423.3032\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 2021.2747 - mae: 34.7910 - mse: 2012.5691 - val_loss: 5643.2603 - val_mae: 29.2979 - val_mse: 5634.5391\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 2076.3145 - mae: 35.5856 - mse: 2067.5876 - val_loss: 5749.1260 - val_mae: 28.8366 - val_mse: 5740.3945\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 1910.5560 - mae: 34.0919 - mse: 1901.8240 - val_loss: 5823.7773 - val_mae: 28.5359 - val_mse: 5815.0430\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1943.5590 - mae: 34.4245 - mse: 1934.8245 - val_loss: 5895.6265 - val_mae: 28.3842 - val_mse: 5886.8940\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1874.8485 - mae: 33.9728 - mse: 1866.1145 - val_loss: 5758.4712 - val_mae: 28.1739 - val_mse: 5749.7334\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1825.9081 - mae: 33.3167 - mse: 1817.1705 - val_loss: 5771.7900 - val_mae: 28.0450 - val_mse: 5763.0498\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1716.4119 - mae: 32.5409 - mse: 1707.6689 - val_loss: 5799.6240 - val_mae: 27.9790 - val_mse: 5790.8750\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1852.3065 - mae: 33.6105 - mse: 1843.5579 - val_loss: 5740.5420 - val_mae: 27.8790 - val_mse: 5731.7905\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1787.4232 - mae: 33.2126 - mse: 1778.6691 - val_loss: 5661.8579 - val_mae: 27.8155 - val_mse: 5653.0991\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1726.5696 - mae: 32.6349 - mse: 1717.8092 - val_loss: 5654.2007 - val_mae: 27.7548 - val_mse: 5645.4360\n",
      "Epoch 20/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1765.0018 - mae: 32.9354 - mse: 1756.2385 - val_loss: 5630.5361 - val_mae: 27.7108 - val_mse: 5621.7632\n",
      "Epoch 21/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1779.8250 - mae: 32.9975 - mse: 1771.0537 - val_loss: 5528.3081 - val_mae: 27.6120 - val_mse: 5519.5337\n",
      "Epoch 22/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1711.9064 - mae: 32.5438 - mse: 1703.1288 - val_loss: 5538.2705 - val_mae: 27.6039 - val_mse: 5529.4883\n",
      "Epoch 23/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 1712.2010 - mae: 32.4734 - mse: 1703.4196 - val_loss: 5527.5278 - val_mae: 27.6151 - val_mse: 5518.7388\n",
      "Epoch 24/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1642.1285 - mae: 31.8193 - mse: 1633.3385 - val_loss: 5353.6948 - val_mae: 27.4994 - val_mse: 5344.8989\n",
      "Epoch 25/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1675.9388 - mae: 32.2604 - mse: 1667.1396 - val_loss: 5453.6226 - val_mae: 27.5116 - val_mse: 5444.8188\n",
      "Epoch 26/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1666.6941 - mae: 31.9377 - mse: 1657.8903 - val_loss: 5383.3066 - val_mae: 27.4233 - val_mse: 5374.5034\n",
      "Epoch 27/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1718.8583 - mae: 32.7958 - mse: 1710.0522 - val_loss: 5234.0420 - val_mae: 27.4216 - val_mse: 5225.2280\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 17:39:52 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 17:40:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 17:40:10 INFO mlflow.tracking._tracking_service.client: 🏃 View run funny-foal-15 at: http://localhost:5000/#/experiments/502514876400363389/runs/7837664fac30460cb4b612db8548c418.\n",
      "2024/11/26 17:40:10 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/502514876400363389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"tensorflow-regression-model\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00015),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.00015)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.4)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso 2: Tasa de aprendizaje el doble de la inicial: 0.0006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 56246.6445 - mae: 233.3169 - mse: 56239.8711 - val_loss: 39505.1914 - val_mae: 190.9235 - val_mse: 39497.8125\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 23692.8164 - mae: 138.8173 - mse: 23684.9609 - val_loss: 11251.9170 - val_mae: 68.8865 - val_mse: 11242.8086\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 2529.1804 - mae: 37.2855 - mse: 2520.0173 - val_loss: 6537.6548 - val_mae: 40.8732 - val_mse: 6528.4380\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 2020.5226 - mae: 34.8194 - mse: 2011.2985 - val_loss: 5237.6611 - val_mae: 30.6112 - val_mse: 5228.3745\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 1781.5146 - mae: 32.8047 - mse: 1772.2214 - val_loss: 4759.3960 - val_mae: 28.2224 - val_mse: 4750.0703\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 1897.1104 - mae: 34.0671 - mse: 1887.7760 - val_loss: 4338.5645 - val_mae: 27.6048 - val_mse: 4329.1826\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 1673.2556 - mae: 32.0727 - mse: 1663.8625 - val_loss: 4069.2214 - val_mae: 27.4450 - val_mse: 4059.8147\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 1733.3854 - mae: 32.6813 - mse: 1723.9585 - val_loss: 3537.6470 - val_mae: 27.6693 - val_mse: 3528.1782\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 1764.2277 - mae: 32.8983 - mse: 1754.7488 - val_loss: 3440.3223 - val_mae: 27.2132 - val_mse: 3430.8142\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 1719.3153 - mae: 32.5971 - mse: 1709.7983 - val_loss: 3295.6572 - val_mae: 27.2265 - val_mse: 3286.0969\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 1761.5270 - mae: 32.8548 - mse: 1751.9718 - val_loss: 3198.3171 - val_mae: 27.1790 - val_mse: 3188.7454\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - loss: 1546.0461 - mae: 30.9108 - mse: 1536.4532 - val_loss: 2875.4365 - val_mae: 27.1715 - val_mse: 2865.7881\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 1565.3193 - mae: 31.0844 - mse: 1555.6698 - val_loss: 2837.2410 - val_mae: 27.0580 - val_mse: 2827.5701\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1604.4340 - mae: 31.5042 - mse: 1594.7649 - val_loss: 2532.5178 - val_mae: 27.2662 - val_mse: 2522.7971\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1594.9117 - mae: 31.4178 - mse: 1585.1832 - val_loss: 2430.2122 - val_mae: 27.4862 - val_mse: 2420.4517\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1590.8323 - mae: 31.4619 - mse: 1581.0730 - val_loss: 2511.1917 - val_mae: 26.7108 - val_mse: 2501.4255\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 1551.6522 - mae: 30.9506 - mse: 1541.8716 - val_loss: 2705.3699 - val_mae: 27.2108 - val_mse: 2695.5811\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 1614.6838 - mae: 31.5044 - mse: 1604.8834 - val_loss: 2425.0471 - val_mae: 26.8202 - val_mse: 2415.2229\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 1579.4902 - mae: 31.3386 - mse: 1569.6460 - val_loss: 2505.9211 - val_mae: 26.9260 - val_mse: 2496.0308\n",
      "Epoch 20/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 1568.9204 - mae: 31.0434 - mse: 1559.0272 - val_loss: 2399.0811 - val_mae: 26.7809 - val_mse: 2389.1848\n",
      "Epoch 21/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 1450.1660 - mae: 29.9139 - mse: 1440.2485 - val_loss: 2546.4802 - val_mae: 27.4119 - val_mse: 2536.5610\n",
      "Epoch 22/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1549.5825 - mae: 31.0113 - mse: 1539.6395 - val_loss: 2275.0679 - val_mae: 26.8605 - val_mse: 2265.0601\n",
      "Epoch 23/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1531.2373 - mae: 30.9206 - mse: 1521.2373 - val_loss: 2148.3464 - val_mae: 26.6399 - val_mse: 2138.3157\n",
      "Epoch 24/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 1529.4203 - mae: 30.6829 - mse: 1519.3679 - val_loss: 2182.9102 - val_mae: 26.7123 - val_mse: 2172.8440\n",
      "Epoch 25/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 1494.4849 - mae: 30.4971 - mse: 1484.4049 - val_loss: 2257.0620 - val_mae: 26.6877 - val_mse: 2246.9426\n",
      "Epoch 26/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 1416.9486 - mae: 29.6050 - mse: 1406.8237 - val_loss: 2211.2583 - val_mae: 26.6667 - val_mse: 2201.0952\n",
      "Epoch 27/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 1431.3514 - mae: 29.7225 - mse: 1421.1906 - val_loss: 2088.6643 - val_mae: 26.7484 - val_mse: 2078.4434\n",
      "Epoch 28/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1525.9915 - mae: 30.7854 - mse: 1515.7755 - val_loss: 2223.4021 - val_mae: 26.8455 - val_mse: 2213.1855\n",
      "Epoch 29/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 1457.5516 - mae: 29.8404 - mse: 1447.3008 - val_loss: 2290.6501 - val_mae: 27.1293 - val_mse: 2280.3774\n",
      "Epoch 30/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 1416.6370 - mae: 29.5284 - mse: 1406.3309 - val_loss: 2201.0129 - val_mae: 26.6871 - val_mse: 2190.6562\n",
      "Epoch 31/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 1422.6271 - mae: 29.7180 - mse: 1412.2732 - val_loss: 2260.4919 - val_mae: 26.9009 - val_mse: 2250.1208\n",
      "Epoch 32/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1426.1626 - mae: 29.6542 - mse: 1415.7595 - val_loss: 2221.2517 - val_mae: 26.7753 - val_mse: 2210.7969\n",
      "Epoch 33/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 1395.5509 - mae: 29.4952 - mse: 1385.0980 - val_loss: 2282.9861 - val_mae: 27.1373 - val_mse: 2272.5037\n",
      "Epoch 34/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1412.1985 - mae: 29.4769 - mse: 1401.7061 - val_loss: 2073.5518 - val_mae: 26.7590 - val_mse: 2063.0088\n",
      "Epoch 35/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1420.4567 - mae: 29.5200 - mse: 1409.8993 - val_loss: 2073.1611 - val_mae: 26.6696 - val_mse: 2062.5710\n",
      "Epoch 36/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1383.8610 - mae: 29.1787 - mse: 1373.2529 - val_loss: 2045.5217 - val_mae: 26.8610 - val_mse: 2034.8671\n",
      "Epoch 37/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1376.0171 - mae: 29.2568 - mse: 1365.3573 - val_loss: 2144.0183 - val_mae: 26.7175 - val_mse: 2133.3308\n",
      "Epoch 38/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1411.5985 - mae: 29.4577 - mse: 1400.8883 - val_loss: 2141.3762 - val_mae: 26.7965 - val_mse: 2130.6382\n",
      "Epoch 39/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1410.3359 - mae: 29.3836 - mse: 1399.5734 - val_loss: 2090.3513 - val_mae: 26.7408 - val_mse: 2079.5461\n",
      "Epoch 40/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1428.0443 - mae: 29.7349 - mse: 1417.2260 - val_loss: 2116.7559 - val_mae: 26.8238 - val_mse: 2105.8860\n",
      "Epoch 41/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1400.8574 - mae: 29.3058 - mse: 1389.9701 - val_loss: 2004.3065 - val_mae: 26.7804 - val_mse: 1993.3802\n",
      "Epoch 42/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1392.2015 - mae: 29.2083 - mse: 1381.2588 - val_loss: 2120.3274 - val_mae: 26.8008 - val_mse: 2109.3589\n",
      "Epoch 43/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 1387.4280 - mae: 29.2023 - mse: 1376.4445 - val_loss: 2082.5356 - val_mae: 26.7227 - val_mse: 2071.5027\n",
      "Epoch 44/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1347.5779 - mae: 28.9052 - mse: 1336.5300 - val_loss: 2066.4167 - val_mae: 26.7944 - val_mse: 2055.3350\n",
      "Epoch 45/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1361.1813 - mae: 28.9974 - mse: 1350.0834 - val_loss: 2009.2860 - val_mae: 26.6599 - val_mse: 1998.1733\n",
      "Epoch 46/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1385.3936 - mae: 29.0679 - mse: 1374.2462 - val_loss: 2093.1152 - val_mae: 26.9108 - val_mse: 2081.9180\n",
      "Epoch 47/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1339.3542 - mae: 28.6873 - mse: 1328.1416 - val_loss: 2227.7715 - val_mae: 26.9988 - val_mse: 2216.5146\n",
      "Epoch 48/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1315.9430 - mae: 28.5270 - mse: 1304.6898 - val_loss: 2182.5095 - val_mae: 26.8514 - val_mse: 2171.2263\n",
      "Epoch 49/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1381.4227 - mae: 29.2574 - mse: 1370.1259 - val_loss: 2116.5874 - val_mae: 26.8155 - val_mse: 2105.2732\n",
      "Epoch 50/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 1336.2859 - mae: 28.7816 - mse: 1324.9484 - val_loss: 2309.2820 - val_mae: 27.0672 - val_mse: 2297.9072\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 17:46:38 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 17:47:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 17:47:02 INFO mlflow.tracking._tracking_service.client: 🏃 View run Tasa 0.0006 at: http://localhost:5000/#/experiments/502514876400363389/runs/8dc0ec19c48946ad84cf854d16eff13c.\n",
      "2024/11/26 17:47:02 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/502514876400363389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"Modelo Cambiando Tasa de Aprendizaje\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0006),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.0006)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.4)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMBIO DROPOUT RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout a la mitad: 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 57985.1133 - mae: 237.0863 - mse: 57978.4766 - val_loss: 54068.2148 - val_mae: 228.6185 - val_mse: 54061.4141\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 45229.3711 - mae: 207.9488 - mse: 45222.4609 - val_loss: 25894.8477 - val_mae: 152.8870 - val_mse: 25887.3945\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 16396.4570 - mae: 119.5791 - mse: 16388.7832 - val_loss: 6094.3481 - val_mae: 61.6161 - val_mse: 6086.0488\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3394.2039 - mae: 46.6882 - mse: 3385.7556 - val_loss: 3086.3020 - val_mae: 29.1884 - val_mse: 3077.5525\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1626.9128 - mae: 31.6778 - mse: 1618.1296 - val_loss: 3010.1011 - val_mae: 27.5591 - val_mse: 3001.2520\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1625.2810 - mae: 31.7080 - mse: 1616.4279 - val_loss: 2994.0740 - val_mae: 27.2976 - val_mse: 2985.2146\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1563.6649 - mae: 30.9389 - mse: 1554.8040 - val_loss: 2943.3567 - val_mae: 27.3075 - val_mse: 2934.4902\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1552.0327 - mae: 31.0517 - mse: 1543.1665 - val_loss: 2865.4390 - val_mae: 27.1379 - val_mse: 2856.5691\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1521.0530 - mae: 30.7574 - mse: 1512.1782 - val_loss: 2777.4636 - val_mae: 27.0861 - val_mse: 2768.5835\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1437.6321 - mae: 29.9946 - mse: 1428.7482 - val_loss: 2685.1123 - val_mae: 27.0999 - val_mse: 2676.2219\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 1430.8933 - mae: 29.7030 - mse: 1422.0004 - val_loss: 2701.2676 - val_mae: 27.1122 - val_mse: 2692.3701\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 1499.0593 - mae: 30.4281 - mse: 1490.1627 - val_loss: 2475.9744 - val_mae: 27.0083 - val_mse: 2467.0645\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1423.9246 - mae: 29.6845 - mse: 1415.0145 - val_loss: 2549.6731 - val_mae: 27.0964 - val_mse: 2540.7686\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1338.4438 - mae: 28.7438 - mse: 1329.5363 - val_loss: 2411.8362 - val_mae: 26.9932 - val_mse: 2402.9172\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1331.3068 - mae: 28.7016 - mse: 1322.3831 - val_loss: 2374.2844 - val_mae: 26.9204 - val_mse: 2365.3535\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1416.9264 - mae: 29.7275 - mse: 1407.9951 - val_loss: 2355.9812 - val_mae: 26.9083 - val_mse: 2347.0442\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1379.8269 - mae: 29.0625 - mse: 1370.8895 - val_loss: 2288.7732 - val_mae: 26.9466 - val_mse: 2279.8223\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1417.7515 - mae: 29.5870 - mse: 1408.8015 - val_loss: 2260.3542 - val_mae: 26.8245 - val_mse: 2251.3999\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1450.8486 - mae: 30.1406 - mse: 1441.8950 - val_loss: 2261.4353 - val_mae: 26.8041 - val_mse: 2252.4751\n",
      "Epoch 20/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1418.5648 - mae: 29.7339 - mse: 1409.6060 - val_loss: 2237.9141 - val_mae: 26.8645 - val_mse: 2228.9404\n",
      "Epoch 21/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1481.3169 - mae: 30.5067 - mse: 1472.3430 - val_loss: 2177.4177 - val_mae: 26.8448 - val_mse: 2168.4434\n",
      "Epoch 22/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1457.2024 - mae: 29.9632 - mse: 1448.2264 - val_loss: 2175.8958 - val_mae: 26.7783 - val_mse: 2166.9114\n",
      "Epoch 23/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1350.4268 - mae: 28.8688 - mse: 1341.4391 - val_loss: 2220.9563 - val_mae: 27.0603 - val_mse: 2211.9685\n",
      "Epoch 24/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1360.2250 - mae: 28.9148 - mse: 1351.2345 - val_loss: 2134.0464 - val_mae: 26.9167 - val_mse: 2125.0454\n",
      "Epoch 25/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1328.6216 - mae: 28.6964 - mse: 1319.6180 - val_loss: 2153.1113 - val_mae: 26.7854 - val_mse: 2144.1074\n",
      "Epoch 26/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 1321.4559 - mae: 28.5901 - mse: 1312.4420 - val_loss: 2130.8208 - val_mae: 26.8073 - val_mse: 2121.8140\n",
      "Epoch 27/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1321.1748 - mae: 28.6662 - mse: 1312.1549 - val_loss: 2150.5408 - val_mae: 26.9040 - val_mse: 2141.5308\n",
      "Epoch 28/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1349.2822 - mae: 28.9333 - mse: 1340.2651 - val_loss: 2188.4695 - val_mae: 27.2196 - val_mse: 2179.4343\n",
      "Epoch 29/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1397.1017 - mae: 29.4463 - mse: 1388.0719 - val_loss: 2131.6946 - val_mae: 26.9402 - val_mse: 2122.6611\n",
      "Epoch 30/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 1324.9210 - mae: 28.5938 - mse: 1315.8840 - val_loss: 2051.3540 - val_mae: 26.8379 - val_mse: 2042.2993\n",
      "Epoch 31/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1385.1896 - mae: 29.3427 - mse: 1376.1356 - val_loss: 2062.9802 - val_mae: 26.6840 - val_mse: 2053.9290\n",
      "Epoch 32/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1310.5934 - mae: 28.4031 - mse: 1301.5393 - val_loss: 2055.1931 - val_mae: 26.8819 - val_mse: 2046.1292\n",
      "Epoch 33/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1371.2970 - mae: 29.2813 - mse: 1362.2302 - val_loss: 2061.0249 - val_mae: 26.8346 - val_mse: 2051.9551\n",
      "Epoch 34/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1270.9922 - mae: 28.1190 - mse: 1261.9128 - val_loss: 2117.5378 - val_mae: 27.0522 - val_mse: 2108.4692\n",
      "Epoch 35/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1309.3232 - mae: 28.5220 - mse: 1300.2462 - val_loss: 2048.4526 - val_mae: 26.8296 - val_mse: 2039.3679\n",
      "Epoch 36/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 1395.4775 - mae: 29.4761 - mse: 1386.3903 - val_loss: 2075.0933 - val_mae: 26.9247 - val_mse: 2065.9934\n",
      "Epoch 37/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 1276.9603 - mae: 28.0993 - mse: 1267.8583 - val_loss: 2083.9233 - val_mae: 26.8661 - val_mse: 2074.8247\n",
      "Epoch 38/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1296.7195 - mae: 28.4237 - mse: 1287.6191 - val_loss: 2030.6652 - val_mae: 26.8413 - val_mse: 2021.5479\n",
      "Epoch 39/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1344.9440 - mae: 28.8513 - mse: 1335.8252 - val_loss: 2034.9353 - val_mae: 26.7960 - val_mse: 2025.8276\n",
      "Epoch 40/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1236.0913 - mae: 27.5310 - mse: 1226.9747 - val_loss: 1942.6459 - val_mae: 26.8201 - val_mse: 1933.5145\n",
      "Epoch 41/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1331.5742 - mae: 28.6103 - mse: 1322.4380 - val_loss: 1939.9979 - val_mae: 26.8239 - val_mse: 1930.8573\n",
      "Epoch 42/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1240.0024 - mae: 27.6464 - mse: 1230.8589 - val_loss: 2037.2725 - val_mae: 26.9566 - val_mse: 2028.1235\n",
      "Epoch 43/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 1370.1084 - mae: 29.0349 - mse: 1360.9628 - val_loss: 1971.1132 - val_mae: 26.9790 - val_mse: 1961.9355\n",
      "Epoch 44/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1240.3407 - mae: 27.7524 - mse: 1231.1722 - val_loss: 1988.0901 - val_mae: 26.8075 - val_mse: 1978.9178\n",
      "Epoch 45/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1326.4731 - mae: 28.7275 - mse: 1317.3054 - val_loss: 2006.9835 - val_mae: 26.9447 - val_mse: 1997.8112\n",
      "Epoch 46/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1338.9042 - mae: 28.8097 - mse: 1329.7285 - val_loss: 1964.5106 - val_mae: 26.8159 - val_mse: 1955.3179\n",
      "Epoch 47/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1268.9443 - mae: 28.0901 - mse: 1259.7494 - val_loss: 1959.2162 - val_mae: 26.8008 - val_mse: 1950.0159\n",
      "Epoch 48/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1291.3361 - mae: 28.3967 - mse: 1282.1354 - val_loss: 1891.8462 - val_mae: 26.7816 - val_mse: 1882.6256\n",
      "Epoch 49/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1286.0930 - mae: 28.3610 - mse: 1276.8727 - val_loss: 1958.1548 - val_mae: 26.9590 - val_mse: 1948.9275\n",
      "Epoch 50/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1321.1631 - mae: 28.6443 - mse: 1311.9324 - val_loss: 1963.2365 - val_mae: 26.8632 - val_mse: 1954.0056\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 18:06:01 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 18:06:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 18:06:23 INFO mlflow.tracking._tracking_service.client: 🏃 View run lyrical-fox-89 at: http://localhost:5000/#/experiments/782055891845388137/runs/f4d706922def4ed8abbdde131ac2879e.\n",
      "2024/11/26 18:06:23 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/782055891845388137.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"Modelo Cambiando Dropout\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.0003)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.2)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El doble del Dropout 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - loss: 59359.0039 - mae: 239.7450 - mse: 59352.3672 - val_loss: 58244.3594 - val_mae: 237.5428 - val_mse: 58237.6133\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 55976.0234 - mae: 232.7307 - mse: 55969.2109 - val_loss: 55204.6172 - val_mae: 231.2492 - val_mse: 55197.5117\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 51036.2695 - mae: 222.1382 - mse: 51029.0352 - val_loss: 46940.4688 - val_mae: 212.9865 - val_mse: 46932.7891\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38993.5703 - mae: 191.2639 - mse: 38985.6914 - val_loss: 25139.9707 - val_mae: 142.3749 - val_mse: 25131.4785\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 20632.6426 - mae: 123.4410 - mse: 20623.9688 - val_loss: 10728.1494 - val_mae: 77.6133 - val_mse: 10718.9844\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 10435.7822 - mae: 80.8869 - mse: 10426.4805 - val_loss: 6954.6069 - val_mae: 50.2692 - val_mse: 6944.9639\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 6393.0713 - mae: 62.3790 - mse: 6383.3555 - val_loss: 5690.3862 - val_mae: 41.2371 - val_mse: 5680.5303\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 4916.7139 - mae: 55.1459 - mse: 4906.8325 - val_loss: 4661.5176 - val_mae: 36.5768 - val_mse: 4651.5776\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 4147.9229 - mae: 50.8154 - mse: 4137.9697 - val_loss: 4001.6909 - val_mae: 33.8319 - val_mse: 3991.7048\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 3840.5625 - mae: 48.7862 - mse: 3830.5676 - val_loss: 3383.0273 - val_mae: 32.0227 - val_mse: 3373.0034\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3612.5117 - mae: 47.4578 - mse: 3602.4868 - val_loss: 3131.1609 - val_mae: 30.7966 - val_mse: 3121.1179\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 3445.1997 - mae: 46.5815 - mse: 3435.1538 - val_loss: 2783.7373 - val_mae: 30.2155 - val_mse: 2773.6660\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3307.6240 - mae: 45.2922 - mse: 3297.5513 - val_loss: 2647.0769 - val_mae: 29.7757 - val_mse: 2636.9963\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 3172.6724 - mae: 44.5190 - mse: 3162.5876 - val_loss: 2509.2903 - val_mae: 29.1881 - val_mse: 2499.1807\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3111.9595 - mae: 44.0509 - mse: 3101.8442 - val_loss: 2385.0693 - val_mae: 28.9680 - val_mse: 2374.9348\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3020.4624 - mae: 43.3666 - mse: 3010.3267 - val_loss: 2305.8374 - val_mae: 28.6946 - val_mse: 2295.6851\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3076.7324 - mae: 44.0473 - mse: 3066.5701 - val_loss: 2203.8494 - val_mae: 28.5776 - val_mse: 2193.6719\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 2978.3220 - mae: 43.1389 - mse: 2968.1370 - val_loss: 2071.4045 - val_mae: 28.4203 - val_mse: 2061.1880\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 2897.5962 - mae: 42.6068 - mse: 2887.3738 - val_loss: 2019.4108 - val_mae: 28.5161 - val_mse: 2009.1803\n",
      "Epoch 20/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2822.8606 - mae: 41.8691 - mse: 2812.6206 - val_loss: 1982.0386 - val_mae: 28.4171 - val_mse: 1971.7925\n",
      "Epoch 21/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2942.6492 - mae: 42.9138 - mse: 2932.3914 - val_loss: 1937.4900 - val_mae: 28.1636 - val_mse: 1927.2107\n",
      "Epoch 22/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2792.6548 - mae: 41.7872 - mse: 2782.3674 - val_loss: 1879.4780 - val_mae: 28.0878 - val_mse: 1869.1713\n",
      "Epoch 23/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 2954.5146 - mae: 42.9019 - mse: 2944.2043 - val_loss: 1815.6666 - val_mae: 27.9806 - val_mse: 1805.3250\n",
      "Epoch 24/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2893.0540 - mae: 42.4461 - mse: 2882.7070 - val_loss: 1788.3053 - val_mae: 28.0748 - val_mse: 1777.9388\n",
      "Epoch 25/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2798.8364 - mae: 41.8602 - mse: 2788.4678 - val_loss: 1750.4304 - val_mae: 27.8869 - val_mse: 1740.0397\n",
      "Epoch 26/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2769.6770 - mae: 41.6421 - mse: 2759.2771 - val_loss: 1753.1487 - val_mae: 28.1849 - val_mse: 1742.7498\n",
      "Epoch 27/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2750.4563 - mae: 41.5838 - mse: 2740.0530 - val_loss: 1690.4897 - val_mae: 27.7328 - val_mse: 1680.0499\n",
      "Epoch 28/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2821.1667 - mae: 42.1491 - mse: 2810.7207 - val_loss: 1695.9215 - val_mae: 28.0622 - val_mse: 1685.4769\n",
      "Epoch 29/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2750.0200 - mae: 41.3854 - mse: 2739.5637 - val_loss: 1679.8864 - val_mae: 28.0986 - val_mse: 1669.4298\n",
      "Epoch 30/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 2722.2241 - mae: 41.2290 - mse: 2711.7524 - val_loss: 1672.4794 - val_mae: 28.2449 - val_mse: 1662.0128\n",
      "Epoch 31/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2705.9326 - mae: 41.1025 - mse: 2695.4529 - val_loss: 1617.9667 - val_mae: 28.1210 - val_mse: 1607.4622\n",
      "Epoch 32/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2716.1443 - mae: 41.2361 - mse: 2705.6355 - val_loss: 1576.6353 - val_mae: 28.1865 - val_mse: 1566.0839\n",
      "Epoch 33/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2681.6008 - mae: 40.8909 - mse: 2671.0637 - val_loss: 1596.8840 - val_mae: 28.0030 - val_mse: 1586.3392\n",
      "Epoch 34/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2794.1187 - mae: 41.7735 - mse: 2783.5686 - val_loss: 1557.3628 - val_mae: 28.0489 - val_mse: 1546.7817\n",
      "Epoch 35/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 2591.8782 - mae: 40.1708 - mse: 2581.3008 - val_loss: 1556.5704 - val_mae: 28.1233 - val_mse: 1545.9845\n",
      "Epoch 36/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 2670.5256 - mae: 41.0069 - mse: 2659.9309 - val_loss: 1538.6200 - val_mae: 27.8493 - val_mse: 1528.0137\n",
      "Epoch 37/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 2604.0486 - mae: 40.4033 - mse: 2593.4402 - val_loss: 1518.0385 - val_mae: 27.7309 - val_mse: 1507.4189\n",
      "Epoch 38/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 2582.6218 - mae: 40.1906 - mse: 2572.0039 - val_loss: 1533.8086 - val_mae: 28.2142 - val_mse: 1523.1935\n",
      "Epoch 39/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 2616.0461 - mae: 40.3867 - mse: 2605.4243 - val_loss: 1485.1899 - val_mae: 27.7929 - val_mse: 1474.5358\n",
      "Epoch 40/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 2597.9858 - mae: 40.4197 - mse: 2587.3408 - val_loss: 1508.3855 - val_mae: 28.0966 - val_mse: 1497.7347\n",
      "Epoch 41/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 2578.0476 - mae: 40.2776 - mse: 2567.3911 - val_loss: 1496.2109 - val_mae: 28.2627 - val_mse: 1485.5321\n",
      "Epoch 42/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 2534.6646 - mae: 39.7793 - mse: 2523.9873 - val_loss: 1473.8181 - val_mae: 27.9354 - val_mse: 1463.1240\n",
      "Epoch 43/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 2568.8767 - mae: 39.9701 - mse: 2558.1746 - val_loss: 1468.8751 - val_mae: 27.9211 - val_mse: 1458.1663\n",
      "Epoch 44/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 2605.5459 - mae: 40.2457 - mse: 2594.8413 - val_loss: 1458.4712 - val_mae: 27.8404 - val_mse: 1447.7513\n",
      "Epoch 45/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 2593.6160 - mae: 40.3536 - mse: 2582.8982 - val_loss: 1470.8823 - val_mae: 28.0239 - val_mse: 1460.1672\n",
      "Epoch 46/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 2476.5098 - mae: 39.2975 - mse: 2465.7800 - val_loss: 1465.2661 - val_mae: 27.9228 - val_mse: 1454.5359\n",
      "Epoch 47/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 2560.1321 - mae: 40.1727 - mse: 2549.3977 - val_loss: 1444.4963 - val_mae: 28.0752 - val_mse: 1433.7412\n",
      "Epoch 48/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 2542.2285 - mae: 39.9855 - mse: 2531.4744 - val_loss: 1449.9857 - val_mae: 28.1274 - val_mse: 1439.2330\n",
      "Epoch 49/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 2464.9590 - mae: 39.0119 - mse: 2454.1968 - val_loss: 1434.3396 - val_mae: 28.0717 - val_mse: 1423.5782\n",
      "Epoch 50/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 2534.1184 - mae: 39.6465 - mse: 2523.3518 - val_loss: 1460.5990 - val_mae: 28.1741 - val_mse: 1449.8368\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/26 18:09:30 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/11/26 18:09:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/26 18:09:45 INFO mlflow.tracking._tracking_service.client: 🏃 View run Dropout 0.8 at: http://localhost:5000/#/experiments/782055891845388137/runs/56cf2f135561425b8ebb20c64c3ae773.\n",
      "2024/11/26 18:09:45 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/782055891845388137.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento registrado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Configurar MLflow para rastrear experimentos\n",
    "mlflow.set_tracking_uri('http://localhost:5000')  # Asegúrate de que tu servidor de MLflow esté activo\n",
    "experiment = mlflow.set_experiment(\"Modelo Cambiando Dropout\")\n",
    "\n",
    "# Escalar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(train_X.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.8),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.8),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Configurar el optimizador y las métricas\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.0003)\n",
    "    mlflow.log_param(\"batch_size\", 256)\n",
    "    mlflow.log_param(\"epochs\", 100)\n",
    "    mlflow.log_param(\"l2_regularization\", 0.01)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.8)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        batch_size=256,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_mae, test_mse = model.evaluate(test_X, test_y, verbose=0)\n",
    "    predictions = model.predict(test_X)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "\n",
    "    # Registrar métricas\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.tensorflow.log_model(model, \"tensorflow-model\")\n",
    "\n",
    "print(\"Experimento registrado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link experimentos: http://127.0.0.1:5000/#/compare-experiments/s?experiments=%5B%22387834654890089351%22%2C%22502514876400363389%22%2C%22581760674512976970%22%2C%22782055891845388137%22%5D&searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
